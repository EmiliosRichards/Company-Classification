{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b367ce59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Company Name for Emails</th>\n",
       "      <th>Account Stage</th>\n",
       "      <th># Employees</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Website</th>\n",
       "      <th>Company Linkedin Url</th>\n",
       "      <th>Facebook Url</th>\n",
       "      <th>Twitter Url</th>\n",
       "      <th>Company Street</th>\n",
       "      <th>...</th>\n",
       "      <th>Number of Retail Locations</th>\n",
       "      <th>Apollo Account Id</th>\n",
       "      <th>SIC Codes</th>\n",
       "      <th>Short Description</th>\n",
       "      <th>Founded Year</th>\n",
       "      <th>Logo Url</th>\n",
       "      <th>Primary Intent Topic</th>\n",
       "      <th>Primary Intent Score</th>\n",
       "      <th>Secondary Intent Topic</th>\n",
       "      <th>Secondary Intent Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trafo Baden │ Congress. Hotels. Catering.</td>\n",
       "      <td>Trafo Baden │ Congress. Hotels. Catering</td>\n",
       "      <td>Cold</td>\n",
       "      <td>27.0</td>\n",
       "      <td>events services</td>\n",
       "      <td>http://www.trafobaden.ch</td>\n",
       "      <td>http://www.linkedin.com/company/trafo-baden---...</td>\n",
       "      <td>https://www.facebook.com/trafobaden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 Brown Boveri Platz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>678aad7fdf3a890001e6f63e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vorhang auf für Ihren Event. Laden Sie ein, sc...</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>https://zenprospect-production.s3.amazonaws.co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OTTO PARTNER ARCHITEKTEN AG</td>\n",
       "      <td>OTTO PARTNER ARCHITEKTEN AG</td>\n",
       "      <td>Cold</td>\n",
       "      <td>24.0</td>\n",
       "      <td>architecture &amp; planning</td>\n",
       "      <td>http://www.ottopartner.ch</td>\n",
       "      <td>http://www.linkedin.com/company/ottopartnerarc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30 Benzburweg</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>678aad81df3a890001e6f690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Architektur - Planung und Bauleitung, Quartier...</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>https://zenprospect-production.s3.amazonaws.co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>treuhand marugg + imsand</td>\n",
       "      <td>treuhand marugg + imsand</td>\n",
       "      <td>Cold</td>\n",
       "      <td>29.0</td>\n",
       "      <td>accounting</td>\n",
       "      <td>http://www.trmi.ch</td>\n",
       "      <td>http://www.linkedin.com/company/treuhand-marug...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 Gliserallee</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>678aad81df3a890001e6f6c0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Treuhand und Immobilien mit Herz und Verstand....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://zenprospect-production.s3.amazonaws.co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SEELAND JOB AG</td>\n",
       "      <td>SEELAND JOB AG</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23.0</td>\n",
       "      <td>staffing &amp; recruiting</td>\n",
       "      <td>http://www.seelandjob.ch</td>\n",
       "      <td>http://www.linkedin.com/company/seeland-job</td>\n",
       "      <td>https://facebook.com/seelandjob</td>\n",
       "      <td>https://twitter.com/seelandjob</td>\n",
       "      <td>107a Aarbergstrasse</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>678aad81df3a890001e6f6c3</td>\n",
       "      <td>7380</td>\n",
       "      <td>WIR SIND IHR REGIONALER JOBPROFI.  \\nDie Firma...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>https://zenprospect-production.s3.amazonaws.co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Burning Box SA</td>\n",
       "      <td>Burning Box SA</td>\n",
       "      <td>Cold</td>\n",
       "      <td>24.0</td>\n",
       "      <td>information technology &amp; services</td>\n",
       "      <td>http://www.burningbox.ch</td>\n",
       "      <td>http://www.linkedin.com/company/burningboxsa</td>\n",
       "      <td>https://www.facebook.com/burningboxsa/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Route Ancienne Papeterie</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>678aad81df3a890001e6f6c7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We focus on our area of expertise: software de...</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>https://zenprospect-production.s3.amazonaws.co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Company  \\\n",
       "0  Trafo Baden │ Congress. Hotels. Catering.   \n",
       "1                OTTO PARTNER ARCHITEKTEN AG   \n",
       "2                   treuhand marugg + imsand   \n",
       "3                             SEELAND JOB AG   \n",
       "4                             Burning Box SA   \n",
       "\n",
       "                    Company Name for Emails Account Stage  # Employees  \\\n",
       "0  Trafo Baden │ Congress. Hotels. Catering          Cold         27.0   \n",
       "1               OTTO PARTNER ARCHITEKTEN AG          Cold         24.0   \n",
       "2                  treuhand marugg + imsand          Cold         29.0   \n",
       "3                            SEELAND JOB AG          Cold         23.0   \n",
       "4                            Burning Box SA          Cold         24.0   \n",
       "\n",
       "                            Industry                    Website  \\\n",
       "0                    events services   http://www.trafobaden.ch   \n",
       "1            architecture & planning  http://www.ottopartner.ch   \n",
       "2                         accounting         http://www.trmi.ch   \n",
       "3              staffing & recruiting   http://www.seelandjob.ch   \n",
       "4  information technology & services   http://www.burningbox.ch   \n",
       "\n",
       "                                Company Linkedin Url  \\\n",
       "0  http://www.linkedin.com/company/trafo-baden---...   \n",
       "1  http://www.linkedin.com/company/ottopartnerarc...   \n",
       "2  http://www.linkedin.com/company/treuhand-marug...   \n",
       "3        http://www.linkedin.com/company/seeland-job   \n",
       "4       http://www.linkedin.com/company/burningboxsa   \n",
       "\n",
       "                             Facebook Url                     Twitter Url  \\\n",
       "0     https://www.facebook.com/trafobaden                             NaN   \n",
       "1                                     NaN                             NaN   \n",
       "2                                     NaN                             NaN   \n",
       "3         https://facebook.com/seelandjob  https://twitter.com/seelandjob   \n",
       "4  https://www.facebook.com/burningboxsa/                             NaN   \n",
       "\n",
       "             Company Street  ... Number of Retail Locations  \\\n",
       "0      1 Brown Boveri Platz  ...                        NaN   \n",
       "1             30 Benzburweg  ...                        NaN   \n",
       "2             1 Gliserallee  ...                        NaN   \n",
       "3       107a Aarbergstrasse  ...                        NaN   \n",
       "4  Route Ancienne Papeterie  ...                        NaN   \n",
       "\n",
       "          Apollo Account Id SIC Codes  \\\n",
       "0  678aad7fdf3a890001e6f63e       NaN   \n",
       "1  678aad81df3a890001e6f690       NaN   \n",
       "2  678aad81df3a890001e6f6c0       NaN   \n",
       "3  678aad81df3a890001e6f6c3      7380   \n",
       "4  678aad81df3a890001e6f6c7       NaN   \n",
       "\n",
       "                                   Short Description Founded Year  \\\n",
       "0  Vorhang auf für Ihren Event. Laden Sie ein, sc...       2003.0   \n",
       "1  Architektur - Planung und Bauleitung, Quartier...       1907.0   \n",
       "2  Treuhand und Immobilien mit Herz und Verstand....          NaN   \n",
       "3  WIR SIND IHR REGIONALER JOBPROFI.  \\nDie Firma...       2008.0   \n",
       "4  We focus on our area of expertise: software de...       1996.0   \n",
       "\n",
       "                                            Logo Url Primary Intent Topic  \\\n",
       "0  https://zenprospect-production.s3.amazonaws.co...                  NaN   \n",
       "1  https://zenprospect-production.s3.amazonaws.co...                  NaN   \n",
       "2  https://zenprospect-production.s3.amazonaws.co...                  NaN   \n",
       "3  https://zenprospect-production.s3.amazonaws.co...                  NaN   \n",
       "4  https://zenprospect-production.s3.amazonaws.co...                  NaN   \n",
       "\n",
       "  Primary Intent Score Secondary Intent Topic  Secondary Intent Score  \n",
       "0                  NaN                    NaN                     NaN  \n",
       "1                  NaN                    NaN                     NaN  \n",
       "2                  NaN                    NaN                     NaN  \n",
       "3                  NaN                    NaN                     NaN  \n",
       "4                  NaN                    NaN                     NaN  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_large = pd.read_csv(r'..\\data\\raw\\Company DACH 15-100 MA Apollo 80k.csv')\n",
    "\n",
    "df_large.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3c204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84896 entries, 0 to 84895\n",
      "Data columns (total 34 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Company                     84896 non-null  object \n",
      " 1   Company Name for Emails     84891 non-null  object \n",
      " 2   Account Stage               84896 non-null  object \n",
      " 3   # Employees                 84889 non-null  float64\n",
      " 4   Industry                    80271 non-null  object \n",
      " 5   Website                     72855 non-null  object \n",
      " 6   Company Linkedin Url        84707 non-null  object \n",
      " 7   Facebook Url                31374 non-null  object \n",
      " 8   Twitter Url                 14157 non-null  object \n",
      " 9   Company Street              63852 non-null  object \n",
      " 10  Company City                84006 non-null  object \n",
      " 11  Company State               84348 non-null  object \n",
      " 12  Company Country             84896 non-null  object \n",
      " 13  Company Postal Code         68711 non-null  float64\n",
      " 14  Company Address             84896 non-null  object \n",
      " 15  Keywords                    47156 non-null  object \n",
      " 16  Company Phone               57336 non-null  object \n",
      " 17  SEO Description             46705 non-null  object \n",
      " 18  Technologies                74607 non-null  object \n",
      " 19  Total Funding               1946 non-null   float64\n",
      " 20  Latest Funding              2387 non-null   object \n",
      " 21  Latest Funding Amount       1823 non-null   float64\n",
      " 22  Last Raised At              2387 non-null   object \n",
      " 23  Annual Revenue              11322 non-null  float64\n",
      " 24  Number of Retail Locations  172 non-null    float64\n",
      " 25  Apollo Account Id           84896 non-null  object \n",
      " 26  SIC Codes                   6038 non-null   object \n",
      " 27  Short Description           71983 non-null  object \n",
      " 28  Founded Year                60530 non-null  float64\n",
      " 29  Logo Url                    84794 non-null  object \n",
      " 30  Primary Intent Topic        0 non-null      float64\n",
      " 31  Primary Intent Score        0 non-null      float64\n",
      " 32  Secondary Intent Topic      0 non-null      float64\n",
      " 33  Secondary Intent Score      0 non-null      float64\n",
      "dtypes: float64(11), object(23)\n",
      "memory usage: 22.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_large.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e832d796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Company Name for Emails</th>\n",
       "      <th>Account Stage</th>\n",
       "      <th># Employees</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Website</th>\n",
       "      <th>Company Linkedin Url</th>\n",
       "      <th>Facebook Url</th>\n",
       "      <th>Twitter Url</th>\n",
       "      <th>Company Street</th>\n",
       "      <th>...</th>\n",
       "      <th>Number of Retail Locations</th>\n",
       "      <th>Apollo Account Id</th>\n",
       "      <th>SIC Codes</th>\n",
       "      <th>Short Description</th>\n",
       "      <th>Founded Year</th>\n",
       "      <th>Logo Url</th>\n",
       "      <th>Primary Intent Topic</th>\n",
       "      <th>Primary Intent Score</th>\n",
       "      <th>Secondary Intent Topic</th>\n",
       "      <th>Secondary Intent Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84896</td>\n",
       "      <td>84891</td>\n",
       "      <td>84896</td>\n",
       "      <td>84889.000000</td>\n",
       "      <td>80271</td>\n",
       "      <td>72855</td>\n",
       "      <td>84707</td>\n",
       "      <td>31374</td>\n",
       "      <td>14157</td>\n",
       "      <td>63852</td>\n",
       "      <td>...</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>84896</td>\n",
       "      <td>6038</td>\n",
       "      <td>71983</td>\n",
       "      <td>60530.000000</td>\n",
       "      <td>84794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>83876</td>\n",
       "      <td>83510</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145</td>\n",
       "      <td>72070</td>\n",
       "      <td>83938</td>\n",
       "      <td>30011</td>\n",
       "      <td>13702</td>\n",
       "      <td>47801</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84896</td>\n",
       "      <td>233</td>\n",
       "      <td>71023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Samina - Ich Schlafe Lebensenergie</td>\n",
       "      <td>NEXUS</td>\n",
       "      <td>Cold</td>\n",
       "      <td>NaN</td>\n",
       "      <td>information technology &amp; services</td>\n",
       "      <td>http://www.remarq.ch</td>\n",
       "      <td>http://www.linkedin.com/company/augmensys</td>\n",
       "      <td>https://www.facebook.com/profile.php</td>\n",
       "      <td>https://twitter.com/privacy</td>\n",
       "      <td>1 Industriestrasse</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>679e693e5e8e940001b79e3e</td>\n",
       "      <td>7380</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://zenprospect-production.s3.amazonaws.co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>84896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10272</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>570</td>\n",
       "      <td>59</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1068</td>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.952750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.901163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1982.239072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.425444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.968962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.849522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1502.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Company Company Name for Emails  \\\n",
       "count                                84896                   84891   \n",
       "unique                               83876                   83510   \n",
       "top     Samina - Ich Schlafe Lebensenergie                   NEXUS   \n",
       "freq                                     4                       4   \n",
       "mean                                   NaN                     NaN   \n",
       "std                                    NaN                     NaN   \n",
       "min                                    NaN                     NaN   \n",
       "25%                                    NaN                     NaN   \n",
       "50%                                    NaN                     NaN   \n",
       "75%                                    NaN                     NaN   \n",
       "max                                    NaN                     NaN   \n",
       "\n",
       "       Account Stage   # Employees                           Industry  \\\n",
       "count          84896  84889.000000                              80271   \n",
       "unique             1           NaN                                145   \n",
       "top             Cold           NaN  information technology & services   \n",
       "freq           84896           NaN                              10272   \n",
       "mean             NaN     38.952750                                NaN   \n",
       "std              NaN     28.425444                                NaN   \n",
       "min              NaN      1.000000                                NaN   \n",
       "25%              NaN     19.000000                                NaN   \n",
       "50%              NaN     28.000000                                NaN   \n",
       "75%              NaN     49.000000                                NaN   \n",
       "max              NaN    420.000000                                NaN   \n",
       "\n",
       "                     Website                       Company Linkedin Url  \\\n",
       "count                  72855                                      84707   \n",
       "unique                 72070                                      83938   \n",
       "top     http://www.remarq.ch  http://www.linkedin.com/company/augmensys   \n",
       "freq                       3                                          3   \n",
       "mean                     NaN                                        NaN   \n",
       "std                      NaN                                        NaN   \n",
       "min                      NaN                                        NaN   \n",
       "25%                      NaN                                        NaN   \n",
       "50%                      NaN                                        NaN   \n",
       "75%                      NaN                                        NaN   \n",
       "max                      NaN                                        NaN   \n",
       "\n",
       "                                Facebook Url                  Twitter Url  \\\n",
       "count                                  31374                        14157   \n",
       "unique                                 30011                        13702   \n",
       "top     https://www.facebook.com/profile.php  https://twitter.com/privacy   \n",
       "freq                                     570                           59   \n",
       "mean                                     NaN                          NaN   \n",
       "std                                      NaN                          NaN   \n",
       "min                                      NaN                          NaN   \n",
       "25%                                      NaN                          NaN   \n",
       "50%                                      NaN                          NaN   \n",
       "75%                                      NaN                          NaN   \n",
       "max                                      NaN                          NaN   \n",
       "\n",
       "            Company Street  ... Number of Retail Locations  \\\n",
       "count                63852  ...                 172.000000   \n",
       "unique               47801  ...                        NaN   \n",
       "top     1 Industriestrasse  ...                        NaN   \n",
       "freq                    45  ...                        NaN   \n",
       "mean                   NaN  ...                   1.901163   \n",
       "std                    NaN  ...                   2.968962   \n",
       "min                    NaN  ...                   1.000000   \n",
       "25%                    NaN  ...                   1.000000   \n",
       "50%                    NaN  ...                   1.000000   \n",
       "75%                    NaN  ...                   2.000000   \n",
       "max                    NaN  ...                  32.000000   \n",
       "\n",
       "               Apollo Account Id SIC Codes  Short Description  Founded Year  \\\n",
       "count                      84896      6038              71983  60530.000000   \n",
       "unique                     84896       233              71023           NaN   \n",
       "top     679e693e5e8e940001b79e3e      7380                  .           NaN   \n",
       "freq                           1      1068                 53           NaN   \n",
       "mean                         NaN       NaN                NaN   1982.239072   \n",
       "std                          NaN       NaN                NaN     43.849522   \n",
       "min                          NaN       NaN                NaN   1502.000000   \n",
       "25%                          NaN       NaN                NaN   1970.000000   \n",
       "50%                          NaN       NaN                NaN   1997.000000   \n",
       "75%                          NaN       NaN                NaN   2011.000000   \n",
       "max                          NaN       NaN                NaN   2024.000000   \n",
       "\n",
       "                                                 Logo Url  \\\n",
       "count                                               84794   \n",
       "unique                                              70877   \n",
       "top     https://zenprospect-production.s3.amazonaws.co...   \n",
       "freq                                                10428   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "       Primary Intent Topic Primary Intent Score Secondary Intent Topic  \\\n",
       "count                   0.0                  0.0                    0.0   \n",
       "unique                  NaN                  NaN                    NaN   \n",
       "top                     NaN                  NaN                    NaN   \n",
       "freq                    NaN                  NaN                    NaN   \n",
       "mean                    NaN                  NaN                    NaN   \n",
       "std                     NaN                  NaN                    NaN   \n",
       "min                     NaN                  NaN                    NaN   \n",
       "25%                     NaN                  NaN                    NaN   \n",
       "50%                     NaN                  NaN                    NaN   \n",
       "75%                     NaN                  NaN                    NaN   \n",
       "max                     NaN                  NaN                    NaN   \n",
       "\n",
       "        Secondary Intent Score  \n",
       "count                      0.0  \n",
       "unique                     NaN  \n",
       "top                        NaN  \n",
       "freq                       NaN  \n",
       "mean                       NaN  \n",
       "std                        NaN  \n",
       "min                        NaN  \n",
       "25%                        NaN  \n",
       "50%                        NaN  \n",
       "75%                        NaN  \n",
       "max                        NaN  \n",
       "\n",
       "[11 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_large.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce155074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Company',\n",
       " 'Company Name for Emails',\n",
       " 'Account Stage',\n",
       " '# Employees',\n",
       " 'Industry',\n",
       " 'Website',\n",
       " 'Company Linkedin Url',\n",
       " 'Facebook Url',\n",
       " 'Twitter Url',\n",
       " 'Company Street',\n",
       " 'Company City',\n",
       " 'Company State',\n",
       " 'Company Country',\n",
       " 'Company Postal Code',\n",
       " 'Company Address',\n",
       " 'Keywords',\n",
       " 'Company Phone',\n",
       " 'SEO Description',\n",
       " 'Technologies',\n",
       " 'Total Funding',\n",
       " 'Latest Funding',\n",
       " 'Latest Funding Amount',\n",
       " 'Last Raised At',\n",
       " 'Annual Revenue',\n",
       " 'Number of Retail Locations',\n",
       " 'Apollo Account Id',\n",
       " 'SIC Codes',\n",
       " 'Short Description',\n",
       " 'Founded Year',\n",
       " 'Logo Url',\n",
       " 'Primary Intent Topic',\n",
       " 'Primary Intent Score',\n",
       " 'Secondary Intent Topic',\n",
       " 'Secondary Intent Score']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_large.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e132af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Logo Url', 'Company Name for Emails', 'Account Stage', 'Number of Retail Locations', 'Apollo Account Id', 'Primary Intent Topic', 'Primary Intent Score', 'Secondary Intent Topic', 'Secondary Intent Score']\n",
    "df_large.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd691fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Company  # Employees  \\\n",
      "2                    treuhand marugg + imsand         29.0   \n",
      "23                         Personal Search AG         19.0   \n",
      "39                                   Feinheit         21.0   \n",
      "44                            Sonnmatt Luzern         27.0   \n",
      "61                           CSDK Architectes         22.0   \n",
      "...                                       ...          ...   \n",
      "84887  Städt. Kliniken Neuss Lukaskrankenhaus         40.0   \n",
      "84888     Zwilag Zwischenlager Würenlingen AG         55.0   \n",
      "84889                                    SSDF         60.0   \n",
      "84890  Volkshochschule Der Hansestadt Rostock         35.0   \n",
      "84892      Ubbo-Emmius-Klinik - Klinik Norden         38.0   \n",
      "\n",
      "                         Industry                       Website  \\\n",
      "2                      accounting            http://www.trmi.ch   \n",
      "23          staffing & recruiting  http://www.personalsearch.ch   \n",
      "39        marketing & advertising        http://www.feinheit.ch   \n",
      "44     health, wellness & fitness        http://www.sonnmatt.ch   \n",
      "61        architecture & planning            http://www.csdk.ch   \n",
      "...                           ...                           ...   \n",
      "84887            medical practice                           NaN   \n",
      "84888   government administration                           NaN   \n",
      "84889                      design                           NaN   \n",
      "84890   government administration                           NaN   \n",
      "84892            medical practice                           NaN   \n",
      "\n",
      "                                    Company Linkedin Url  \\\n",
      "2      http://www.linkedin.com/company/treuhand-marug...   \n",
      "23     http://www.linkedin.com/company/personal-searc...   \n",
      "39              http://www.linkedin.com/company/feinheit   \n",
      "44        http://www.linkedin.com/company/sonnmattluzern   \n",
      "61      http://www.linkedin.com/company/csdk-architectes   \n",
      "...                                                  ...   \n",
      "84887  http://www.linkedin.com/company/st%c3%a4dt.-kl...   \n",
      "84888  http://www.linkedin.com/company/zwilag-zwische...   \n",
      "84889               http://www.linkedin.com/company/ssdf   \n",
      "84890  http://www.linkedin.com/company/volkshochschul...   \n",
      "84892  http://www.linkedin.com/company/ubbo-emmius-kl...   \n",
      "\n",
      "                                    Facebook Url  \\\n",
      "2                                            NaN   \n",
      "23                                           NaN   \n",
      "39                 https://facebook.com/FEINHEIT   \n",
      "44          https://facebook.com/Luzern.Sonnmatt   \n",
      "61     https://www.facebook.com/csdkarchitectes/   \n",
      "...                                          ...   \n",
      "84887                                        NaN   \n",
      "84888                                        NaN   \n",
      "84889                                        NaN   \n",
      "84890                                        NaN   \n",
      "84892                                        NaN   \n",
      "\n",
      "                           Twitter Url                Company Street  \\\n",
      "2                                  NaN                 1 Gliserallee   \n",
      "23                                 NaN  12 Innere Margarethenstrasse   \n",
      "39     http://www.twitter.com/feinheit              54 Fabrikstrasse   \n",
      "44                                 NaN              Hemschlenstrasse   \n",
      "61                                 NaN                           NaN   \n",
      "...                                ...                           ...   \n",
      "84887                              NaN                           NaN   \n",
      "84888                              NaN                             1   \n",
      "84889                              NaN                           NaN   \n",
      "84890                              NaN            20A Am Kabutzenhof   \n",
      "84892                              NaN                           NaN   \n",
      "\n",
      "       Company City           Company State  ...  \\\n",
      "2              Brig                  Valais  ...   \n",
      "23            Basel             Basel-Stadt  ...   \n",
      "39          Zuerich                 Zuerich  ...   \n",
      "44          Lucerne                 Lucerne  ...   \n",
      "61           Geneva                  Geneva  ...   \n",
      "...             ...                     ...  ...   \n",
      "84887         Neuss  North Rhine-Westphalia  ...   \n",
      "84888  Wuerenlingen                  Aargau  ...   \n",
      "84889       Cologne  North Rhine-Westphalia  ...   \n",
      "84890       Rostock  Mecklenburg-Vorpommern  ...   \n",
      "84892        Norden            Lower Saxony  ...   \n",
      "\n",
      "                                         SEO Description  \\\n",
      "2                                                    NaN   \n",
      "23     Personal Search AG ist Ihr zuverlässiger Perso...   \n",
      "39     Seit 2006 sind wir eine Marketing- und Kommuni...   \n",
      "44     Welcome to the Sonnmatt Luzern! Relax, holiday...   \n",
      "61     Le bureau d'architecture CSDK Architectes, bas...   \n",
      "...                                                  ...   \n",
      "84887                                                NaN   \n",
      "84888                                                NaN   \n",
      "84889                                                NaN   \n",
      "84890                                                NaN   \n",
      "84892                                                NaN   \n",
      "\n",
      "                                            Technologies Total Funding  \\\n",
      "2      Outlook, Microsoft Office 365, Google Analytic...           NaN   \n",
      "23     Outlook, Google Font API, Nginx, Google Maps, ...           NaN   \n",
      "39     DNSimple, SendInBlue, Gmail, Google Apps, Mail...           NaN   \n",
      "44     Sendgrid, Outlook, Microsoft Office 365, Vimeo...           NaN   \n",
      "61     Outlook, Microsoft Office 365, Google Maps (No...           NaN   \n",
      "...                                                  ...           ...   \n",
      "84887                                                NaN           NaN   \n",
      "84888                                                NaN           NaN   \n",
      "84889                                                NaN           NaN   \n",
      "84890                                                NaN           NaN   \n",
      "84892                                                NaN           NaN   \n",
      "\n",
      "      Latest Funding Latest Funding Amount Last Raised At Annual Revenue  \\\n",
      "2                NaN                   NaN            NaN            NaN   \n",
      "23               NaN                   NaN            NaN            NaN   \n",
      "39               NaN                   NaN            NaN      2771000.0   \n",
      "44               NaN                   NaN            NaN            NaN   \n",
      "61               NaN                   NaN            NaN            NaN   \n",
      "...              ...                   ...            ...            ...   \n",
      "84887            NaN                   NaN            NaN            NaN   \n",
      "84888            NaN                   NaN            NaN            NaN   \n",
      "84889            NaN                   NaN            NaN            NaN   \n",
      "84890            NaN                   NaN            NaN            NaN   \n",
      "84892            NaN                   NaN            NaN            NaN   \n",
      "\n",
      "       SIC Codes                                  Short Description  \\\n",
      "2            NaN  Treuhand und Immobilien mit Herz und Verstand....   \n",
      "23           NaN  Personal Search AG is a company based out of I...   \n",
      "39          7380  Bei Feinheit treffen sich Weltverbesserer, Spi...   \n",
      "44          7011  Gesund werden, gesund bleiben, gelassen altern...   \n",
      "61           NaN  CSDK Architectes is a Swiss architecture firm ...   \n",
      "...          ...                                                ...   \n",
      "84887        NaN                                                NaN   \n",
      "84888        NaN                                                NaN   \n",
      "84889        NaN                                                NaN   \n",
      "84890        NaN                                                NaN   \n",
      "84892        NaN                                                NaN   \n",
      "\n",
      "       Founded Year  \n",
      "2               NaN  \n",
      "23           2005.0  \n",
      "39           2006.0  \n",
      "44           1910.0  \n",
      "61           2015.0  \n",
      "...             ...  \n",
      "84887           NaN  \n",
      "84888           NaN  \n",
      "84889           NaN  \n",
      "84890           NaN  \n",
      "84892           NaN  \n",
      "\n",
      "[13596 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display duplicated Website rows \n",
    "duplicated_websites = df_large[df_large.duplicated(subset='Website', keep=False)]\n",
    "print(duplicated_websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd77348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website\n",
      "http://www.remarq.ch                  3\n",
      "http://www.hr-andrist.ch              3\n",
      "http://www.teleactis.ch               3\n",
      "http://www.felss.com                  3\n",
      "http://www.hoermann-automotive.com    3\n",
      "                                     ..\n",
      "http://www.trasteel.com               2\n",
      "http://www.globefuelcell.com          2\n",
      "http://www.algorithm-factory.com      2\n",
      "http://www.iqpc.de                    2\n",
      "http://www.shmh.de                    2\n",
      "Name: count, Length: 770, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# display not unique number of Website values\n",
    "not_unique_websites = df_large['Website'].value_counts()\n",
    "print(not_unique_websites[not_unique_websites > 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e73ec0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count before custom 'Website' (with 'company_phone' logic) deduplication: 84896\n",
      "\n",
      "--- Checking df_groups_to_apply_custom_logic for the problematic website ---\n",
      "Found 2 rows for 'http://www.fit4future-foundation.ch' in df_groups_to_apply_custom_logic:\n",
      "                                  Company                              Website      Company Phone\n",
      "39916  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83\n",
      "39948  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83\n",
      "\n",
      "Unique website strings in df_groups_to_apply_custom_logic containing 'fit4future':\n",
      "  - 'http://www.fit4future-foundation.ch' (repr: 'http://www.fit4future-foundation.ch')\n",
      "\n",
      "--- MANUALLY ITERATING GROUPS for apply() ---\n",
      "\n",
      "MANUAL: Found problematic group: http://www.fit4future-foundation.ch\n",
      "MANUAL: Calling resolve_website_duplicates_by_phone_revised_DEBUG for 'http://www.fit4future-foundation.ch'\n",
      "\n",
      "--- DEBUG: Processing group for Website: http://www.fit4future-foundation.ch ---\n",
      "Input group_df (indices: [39916, 39948]):\n",
      "                                  Company                              Website      Company Phone\n",
      "39916  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83\n",
      "39948  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83\n",
      "\n",
      "DEBUG: 'phones_series' (normalized phone numbers) for this group:\n",
      "  Row index 39916: Original=''+41 41 766 63 83' (repr: \"'+41 41 766 63 83\") ==> Normalized=''+41 41 766 63 83' (repr: \"'+41 41 766 63 83\")\n",
      "  Row index 39948: Original=''+41 41 766 63 83' (repr: \"'+41 41 766 63 83\") ==> Normalized=''+41 41 766 63 83' (repr: \"'+41 41 766 63 83\")\n",
      "\n",
      "DEBUG: Rows in 'df_with_non_empty_phones' (original phone numbers from group_df):\n",
      "                                  Company                              Website      Company Phone\n",
      "39916  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83\n",
      "39948  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83\n",
      "\n",
      "DEBUG: 'df_to_dedupe' (with __normalized_phone__) BEFORE drop_duplicates:\n",
      "                                  Company                              Website      Company Phone __normalized_phone__\n",
      "39916  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83    '+41 41 766 63 83\n",
      "39948  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83    '+41 41 766 63 83\n",
      "\n",
      "DEBUG: Details of '__normalized_phone__' column in 'df_to_dedupe':\n",
      "  Row index 39916: Value=''+41 41 766 63 83', Type: <class 'str'>, repr: \"'+41 41 766 63 83\"\n",
      "    Character codes: [39, 43, 52, 49, 32, 52, 49, 32, 55, 54, 54, 32, 54, 51, 32, 56, 51]\n",
      "  Row index 39948: Value=''+41 41 766 63 83', Type: <class 'str'>, repr: \"'+41 41 766 63 83\"\n",
      "    Character codes: [39, 43, 52, 49, 32, 52, 49, 32, 55, 54, 54, 32, 54, 51, 32, 56, 51]\n",
      "\n",
      "DEBUG: 'kept_rows_with_phone' AFTER drop_duplicates:\n",
      "                                  Company                              Website      Company Phone __normalized_phone__\n",
      "39916  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83    '+41 41 766 63 83\n",
      "MANUAL: Result from resolve_website_duplicates_by_phone_revised_DEBUG for 'http://www.fit4future-foundation.ch':\n",
      "                                  Company                              Website      Company Phone\n",
      "39916  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83\n",
      "\n",
      "MANUAL: Number of unique websites processed by groupby: 770\n",
      "MANUAL: 'http://www.fit4future-foundation.ch' WAS processed in the loop.\n",
      "Count after custom 'Website' (with 'company_phone' logic) deduplication: 84121\n",
      "\n",
      "--- Displaying 'Company Phone' for specific rows (AFTER DEBUG RUN) from df_final_result_website_dedup ---\n",
      "                                  Company                              Website      Company Phone\n",
      "39916  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch  '+41 41 766 63 83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Assume 'df_large' is your DataFrame before this new 'Website' deduplication.\n",
    "original_count = len(df_large)\n",
    "print(f\"Count before custom 'Website' (with 'company_phone' logic) deduplication: {original_count}\")\n",
    "\n",
    "# --- Step 1: Separate and preserve rows with initially empty/NaN 'Website' ---\n",
    "empty_website_mask = df_large['Website'].isna() | (df_large['Website'] == '')\n",
    "df_kept_empty_website_rows = df_large[empty_website_mask]\n",
    "df_to_process_non_empty_websites = df_large[~empty_website_mask]\n",
    "\n",
    "# --- Step 2: Process rows with non-empty 'Website' values ---\n",
    "# Identify non-empty websites that are already unique (appear only once)\n",
    "website_counts = df_to_process_non_empty_websites['Website'].value_counts()\n",
    "unique_website_strings = website_counts[website_counts == 1].index.tolist()\n",
    "df_kept_unique_non_empty_website_rows = df_to_process_non_empty_websites[\n",
    "    df_to_process_non_empty_websites['Website'].isin(unique_website_strings)\n",
    "]\n",
    "\n",
    "# Isolate groups of rows where the non-empty 'Website' is duplicated\n",
    "duplicated_website_strings_for_grouping = website_counts[website_counts > 1].index.tolist()\n",
    "df_groups_to_apply_custom_logic = df_to_process_non_empty_websites[\n",
    "    df_to_process_non_empty_websites['Website'].isin(duplicated_website_strings_for_grouping)\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Deduplicate company records by Website and Company Phone.\n",
    "# \n",
    "# 1. Preserves rows with empty/NaN Website.\n",
    "# 2. Keeps unique Website rows as-is.\n",
    "# 3. For duplicated Websites, deduplicates by normalized Company Phone using\n",
    "#    resolve_website_duplicates_by_phone_revised_DEBUG.\n",
    "# 4. Combines all results into df_final_result.\n",
    "# 5. Prints debug info for a specific problematic Website.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- Start: Function with enhanced debugging ---\n",
    "def resolve_website_duplicates_by_phone_revised_DEBUG(group_df):\n",
    "    \"\"\"\n",
    "    Deduplicate a group of rows sharing the same Website by normalized Company Phone.\n",
    "\n",
    "    - If the group has only one row, returns it as-is.\n",
    "    - Normalizes 'Company Phone' values by removing problematic whitespace, zero-width spaces,\n",
    "      and surrounding quotes, and collapsing all whitespace.\n",
    "    - If all normalized phones are empty, keeps only the first row.\n",
    "    - Otherwise, keeps the first occurrence of each unique normalized phone.\n",
    "    - Prints detailed debug output for the Website 'http://www.fit4future-foundation.ch'.\n",
    "\n",
    "    Args:\n",
    "        group_df (pd.DataFrame): DataFrame group with the same Website.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Deduplicated group DataFrame.\n",
    "    \"\"\"\n",
    "    DEBUG_WEBSITE_URL = 'http://www.fit4future-foundation.ch'\n",
    "    # Check if the current group is the one we want to debug\n",
    "    # It's safer to check if 'Website' column exists and group_df is not empty\n",
    "    is_debug_group = False\n",
    "    if not group_df.empty and 'Website' in group_df.columns and group_df['Website'].iloc[0] == DEBUG_WEBSITE_URL:\n",
    "        is_debug_group = True\n",
    "\n",
    "    if is_debug_group:\n",
    "        print(f\"\\n--- DEBUG: Processing group for Website: {DEBUG_WEBSITE_URL} ---\")\n",
    "        print(f\"Input group_df (indices: {group_df.index.tolist()}):\")\n",
    "        print(group_df[['Company', 'Website', 'Company Phone']].to_string())\n",
    "\n",
    "    if len(group_df) == 1:\n",
    "        if is_debug_group: print(\"DEBUG: Group has only 1 row, returning as is.\")\n",
    "        return group_df\n",
    "\n",
    "    def robust_normalize_phone(val):\n",
    "        original_val_for_debug = val # For logging\n",
    "        if pd.isna(val):\n",
    "            return \"\"\n",
    "        s = str(val)\n",
    "        \n",
    "        # Layer 1: Explicitly replace known problematic Unicode whitespace\n",
    "        s_intermediate = s.replace('\\u00A0', ' ') # Non-breaking space\n",
    "        s_intermediate = s_intermediate.replace('\\u200B', '') # Zero-width space\n",
    "        # Add others if suspected (e.g., \\u200C for ZWNJ, \\u200D for ZWJ, \\uFEFF for BOM)\n",
    "        \n",
    "        # Layer 2: Consolidate all remaining whitespace types (std/unicode) to a single space\n",
    "        s_intermediate = re.sub(r'\\s+', ' ', s_intermediate)\n",
    "        \n",
    "        # Layer 3: Strip leading/trailing whitespace\n",
    "        s_normalized = s_intermediate.strip()\n",
    "        \n",
    "        # Layer 4: Optional - Strip surrounding quotes if they are part of the data string\n",
    "        # and not part of the actual phone number content.\n",
    "        # This is speculative but can be a source of \"hidden\" differences.\n",
    "        if len(s_normalized) >= 2 and s_normalized.startswith(\"'\") and s_normalized.endswith(\"'\"):\n",
    "            s_normalized = s_normalized[1:-1].strip() # Strip quotes and re-strip whitespace\n",
    "        if len(s_normalized) >= 2 and s_normalized.startswith('\"') and s_normalized.endswith('\"'):\n",
    "            s_normalized = s_normalized[1:-1].strip() # Strip quotes and re-strip whitespace\n",
    "            \n",
    "        if is_debug_group: # Only print if there was a change or it's the debug group\n",
    "             if s != s_normalized or original_val_for_debug != s_normalized : # helps to see effect\n",
    "                print(f\"DEBUG Normalization Detail: Original='{original_val_for_debug}' (repr: {repr(original_val_for_debug)}) ==> Normalized='{s_normalized}' (repr: {repr(s_normalized)})\")\n",
    "        return s_normalized\n",
    "\n",
    "    # Apply the robust normalization to the 'Company Phone' column\n",
    "    phones_series = group_df['Company Phone'].apply(robust_normalize_phone)\n",
    "    \n",
    "    if is_debug_group:\n",
    "        print(\"\\nDEBUG: 'phones_series' (normalized phone numbers) for this group:\")\n",
    "        for idx, (original_phone, normalized_phone) in enumerate(zip(group_df['Company Phone'], phones_series)):\n",
    "            # Using group_df.index[idx] to get the original index\n",
    "            print(f\"  Row index {group_df.index[idx]}: Original='{original_phone}' (repr: {repr(original_phone)}) ==> Normalized='{normalized_phone}' (repr: {repr(normalized_phone)})\")\n",
    "\n",
    "    has_non_empty_phone_mask = (phones_series != '')\n",
    "    df_with_non_empty_phones = group_df[has_non_empty_phone_mask]\n",
    "    \n",
    "    if df_with_non_empty_phones.empty:\n",
    "        if is_debug_group: print(\"DEBUG: No non-empty phones after normalization for this group. Keeping head(1) of original group.\")\n",
    "        return group_df.head(1)\n",
    "    else:\n",
    "        if is_debug_group:\n",
    "            print(\"\\nDEBUG: Rows in 'df_with_non_empty_phones' (original phone numbers from group_df):\")\n",
    "            print(df_with_non_empty_phones[['Company', 'Website', 'Company Phone']].to_string())\n",
    "\n",
    "        df_to_dedupe = df_with_non_empty_phones.copy()\n",
    "        # Assign the relevant slice of normalized phones\n",
    "        # Ensure correct alignment: phones_series was created from group_df,\n",
    "        # so phones_series[has_non_empty_phone_mask] aligns with df_with_non_empty_phones\n",
    "        df_to_dedupe['__normalized_phone__'] = phones_series[has_non_empty_phone_mask]\n",
    "        \n",
    "        if is_debug_group:\n",
    "            print(\"\\nDEBUG: 'df_to_dedupe' (with __normalized_phone__) BEFORE drop_duplicates:\")\n",
    "            print(df_to_dedupe[['Company', 'Website', 'Company Phone', '__normalized_phone__']].to_string())\n",
    "            print(\"\\nDEBUG: Details of '__normalized_phone__' column in 'df_to_dedupe':\")\n",
    "            for r_idx, phone_val in df_to_dedupe['__normalized_phone__'].items(): # Use .items() to get index too\n",
    "                print(f\"  Row index {r_idx}: Value='{phone_val}', Type: {type(phone_val)}, repr: {repr(phone_val)}\")\n",
    "                print(f\"    Character codes: {[ord(c) for c in str(phone_val)]}\")\n",
    "\n",
    "        # The critical deduplication step\n",
    "        kept_rows_with_phone = df_to_dedupe.drop_duplicates(subset=['__normalized_phone__'], keep='first')\n",
    "        \n",
    "        if is_debug_group:\n",
    "            print(\"\\nDEBUG: 'kept_rows_with_phone' AFTER drop_duplicates:\")\n",
    "            print(kept_rows_with_phone[['Company', 'Website', 'Company Phone', '__normalized_phone__']].to_string())\n",
    "        \n",
    "        return kept_rows_with_phone.drop(columns=['__normalized_phone__'])\n",
    "# --- End: Function with enhanced debugging ---\n",
    "\n",
    "\n",
    "print(\"\\n--- Checking df_groups_to_apply_custom_logic for the problematic website ---\")\n",
    "problematic_website_in_grouping_df = df_groups_to_apply_custom_logic[\n",
    "    df_groups_to_apply_custom_logic['Website'] == 'http://www.fit4future-foundation.ch'\n",
    "]\n",
    "if not problematic_website_in_grouping_df.empty:\n",
    "    print(f\"Found {len(problematic_website_in_grouping_df)} rows for 'http://www.fit4future-foundation.ch' in df_groups_to_apply_custom_logic:\")\n",
    "    print(problematic_website_in_grouping_df[['Company', 'Website', 'Company Phone']].to_string())\n",
    "    \n",
    "    # Also check the raw website strings in df_groups_to_apply_custom_logic\n",
    "    # to ensure there are no hidden characters preventing the match in the debug function.\n",
    "    print(\"\\nUnique website strings in df_groups_to_apply_custom_logic containing 'fit4future':\")\n",
    "    fit4future_websites = df_groups_to_apply_custom_logic[\n",
    "        df_groups_to_apply_custom_logic['Website'].astype(str).str.contains('fit4future', case=False, na=False)\n",
    "    ]['Website'].unique()\n",
    "    for site in fit4future_websites:\n",
    "        print(f\"  - '{site}' (repr: {repr(site)})\")\n",
    "else:\n",
    "    print(\"'http://www.fit4future-foundation.ch' NOT FOUND in df_groups_to_apply_custom_logic.\")\n",
    "    print(f\"Is 'http://www.fit4future-foundation.ch' present in df_to_process_non_empty_websites at all?\")\n",
    "    if 'df_to_process_non_empty_websites' in locals():\n",
    "        temp_check = df_to_process_non_empty_websites[\n",
    "            df_to_process_non_empty_websites['Website'] == 'http://www.fit4future-foundation.ch'\n",
    "        ]\n",
    "        print(f\"  Count in df_to_process_non_empty_websites: {len(temp_check)}\")\n",
    "        if not temp_check.empty:\n",
    "             print(temp_check[['Company', 'Website', 'Company Phone']].to_string())\n",
    "\n",
    "\n",
    "processed_group_dfs = [] # To store results from manual processing\n",
    "processed_websites = set()\n",
    "\n",
    "if not df_groups_to_apply_custom_logic.empty:\n",
    "    print(\"\\n--- MANUALLY ITERATING GROUPS for apply() ---\")\n",
    "    for website_name, group_data in df_groups_to_apply_custom_logic.groupby('Website'):\n",
    "        processed_websites.add(website_name)\n",
    "        if website_name == 'http://www.fit4future-foundation.ch':\n",
    "            print(f\"\\nMANUAL: Found problematic group: {website_name}\")\n",
    "            print(f\"MANUAL: Calling resolve_website_duplicates_by_phone_revised_DEBUG for '{website_name}'\")\n",
    "            # Call the debug function directly\n",
    "            result_df = resolve_website_duplicates_by_phone_revised_DEBUG(group_data.copy()) # Pass a copy\n",
    "            print(f\"MANUAL: Result from resolve_website_duplicates_by_phone_revised_DEBUG for '{website_name}':\")\n",
    "            if result_df is not None and not result_df.empty:\n",
    "                print(result_df[['Company', 'Website', 'Company Phone']].to_string())\n",
    "            elif result_df is not None and result_df.empty:\n",
    "                print(\"MANUAL: Function returned an EMPTY DataFrame.\")\n",
    "            else:\n",
    "                print(\"MANUAL: Function returned None.\")\n",
    "            processed_group_dfs.append(result_df)\n",
    "        else:\n",
    "            # For other groups, call it normally (or the debug version if you want to see all)\n",
    "            # Using the non-debug version here to reduce log noise unless needed\n",
    "            # result_df = resolve_website_duplicates_by_phone_revised(group_data.copy())\n",
    "            # For consistency in debugging, let's keep using the DEBUG one for now.\n",
    "            result_df = resolve_website_duplicates_by_phone_revised_DEBUG(group_data.copy())\n",
    "            processed_group_dfs.append(result_df)\n",
    "\n",
    "    if processed_group_dfs:\n",
    "        df_resolved_website_duplicates = pd.concat(processed_group_dfs)\n",
    "    else:\n",
    "        df_resolved_website_duplicates = pd.DataFrame(columns=df_large.columns) # Or df_groups_to_apply_custom_logic.columns\n",
    "\n",
    "    print(f\"\\nMANUAL: Number of unique websites processed by groupby: {len(processed_websites)}\")\n",
    "    if 'http://www.fit4future-foundation.ch' in processed_websites:\n",
    "        print(\"MANUAL: 'http://www.fit4future-foundation.ch' WAS processed in the loop.\")\n",
    "    else:\n",
    "        print(\"MANUAL: 'http://www.fit4future-foundation.ch' WAS NOT processed in the loop. VERY STRANGE.\")\n",
    "\n",
    "else:\n",
    "    df_resolved_website_duplicates = pd.DataFrame(columns=df_large.columns) # Or df_groups_to_apply_custom_logic.columns\n",
    "\n",
    "# --- Step 3: Combine all processed parts ---\n",
    "df_final_result_website_dedup = pd.concat([\n",
    "    df_kept_empty_website_rows,\n",
    "    df_kept_unique_non_empty_website_rows,\n",
    "    df_resolved_website_duplicates\n",
    "]).sort_index() # Restore original DataFrame order\n",
    "\n",
    "# You can assign this result back to df_large if needed for subsequent steps\n",
    "# df_large = df_final_result_website_dedup # Example\n",
    "\n",
    "final_count = len(df_final_result_website_dedup)\n",
    "print(f\"Count after custom 'Website' (with 'company_phone' logic) deduplication: {final_count}\")\n",
    "# print(f\"Number of rows dropped by this step: {original_count - final_count}\") # if original_count is defined\n",
    "\n",
    "# --- Then, inspect the final problematic rows again ---\n",
    "print(\"\\n--- Displaying 'Company Phone' for specific rows (AFTER DEBUG RUN) from df_final_result_website_dedup ---\")\n",
    "specific_rows_after_debug = df_final_result_website_dedup[df_final_result_website_dedup['Website'] == 'http://www.fit4future-foundation.ch']\n",
    "if not specific_rows_after_debug.empty:\n",
    "    print(specific_rows_after_debug[['Company', 'Website', 'Company Phone']].to_string())\n",
    "else:\n",
    "    print(f\"No rows found for website http://www.fit4future-foundation.ch in the final result.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bfc5f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inspecting 'Company' Duplicates with Varying 'Website' Values ---\n",
      "(This inspection is performed on the DataFrame after 'Website' deduplication but before 'Company' deduplication)\n",
      "\n",
      "Found 100 distinct companies that are duplicated and have different 'Website' values.\n",
      "Displaying these groups (showing 'Company' and 'Website', sorted for clarity):\n",
      "                                  Company                              Website\n",
      "9320                             360 Grad               http://www.360-grad.at\n",
      "56759                            360 Grad                http://www.360-net.de\n",
      "61160                           ACS Group                 http://www.acs-e.com\n",
      "44792                           ACS Group               http://www.acsgroup.de\n",
      "51226                                 AIM               http://www.agile-im.de\n",
      "...                                   ...                                  ...\n",
      "37960                          up to data               http://www.uptodata.de\n",
      "19636  «fit4future»@fit4future foundation           http://www.fit-4-future.ch\n",
      "39916  «fit4future»@fit4future foundation  http://www.fit4future-foundation.ch\n",
      "79696                         فرنسا اليوم           http://www.francealyom.com\n",
      "37784                         فرنسا اليوم           http://www.king-shoot.live\n",
      "\n",
      "[204 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Inspects the deduplicated DataFrame for companies that appear more than once with different Website values.\n",
    "\n",
    "This block:\n",
    "- Runs after Website deduplication but before Company deduplication.\n",
    "- Identifies all companies (non-empty names) that have duplicate entries.\n",
    "- Filters for companies where the same name is associated with multiple unique Website values.\n",
    "- Prints out these cases for manual review, showing the Company and Website columns.\n",
    "- Helps to spot potential data quality issues before further deduplication steps.\n",
    "\n",
    "Intended for exploratory data analysis and quality control.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Assume 'df_final_result_website_dedup' is your DataFrame AFTER selective 'Website' deduplication\n",
    "# and BEFORE you apply selective 'Company' deduplication.\n",
    "\n",
    "print(\"\\n--- Inspecting 'Company' Duplicates with Varying 'Website' Values ---\")\n",
    "print(\"(This inspection is performed on the DataFrame after 'Website' deduplication but before 'Company' deduplication)\\n\")\n",
    "\n",
    "# 1. Consider only rows with non-empty 'Company' names for this inspection\n",
    "non_empty_company_mask_for_inspection = df_final_result_website_dedup['Company'].notna() & (df_final_result_website_dedup['Company'] != '')\n",
    "df_to_inspect = df_final_result_website_dedup[non_empty_company_mask_for_inspection]\n",
    "\n",
    "# 2. Identify all rows that are part of a 'Company' duplicate group\n",
    "#    'keep=False' ensures all occurrences of such duplicates are included\n",
    "all_company_duplicate_occurrences = df_to_inspect[\n",
    "    df_to_inspect.duplicated(subset=['Company'], keep=False)\n",
    "]\n",
    "\n",
    "if not all_company_duplicate_occurrences.empty:\n",
    "    # 3. Group by 'Company' and filter for those groups where the number of unique 'Website' values is > 1\n",
    "    #    This means for a given 'Company' name, there are multiple different 'Website' entries.\n",
    "    company_groups_with_different_websites = all_company_duplicate_occurrences.groupby('Company').filter(\n",
    "        lambda group: group['Website'].nunique() > 1\n",
    "    )\n",
    "\n",
    "    if not company_groups_with_different_websites.empty:\n",
    "        print(f\"Found {company_groups_with_different_websites['Company'].nunique()} distinct companies that are duplicated and have different 'Website' values.\")\n",
    "        print(\"Displaying these groups (showing 'Company' and 'Website', sorted for clarity):\")\n",
    "        \n",
    "        # Select relevant columns and sort for better readability\n",
    "        # You can adjust the number of rows displayed using .head(N) if the output is too long\n",
    "        columns_to_display = ['Company', 'Website'] # Add other columns if they provide useful context\n",
    "        print(company_groups_with_different_websites[columns_to_display].sort_values(by=['Company', 'Website']))\n",
    "        \n",
    "    else:\n",
    "        print(\"No 'Company' duplicate groups were found to have different 'Website' values.\")\n",
    "else:\n",
    "    print(\"No non-empty 'Company' duplicate groups were found in the DataFrame to inspect.\")\n",
    "\n",
    "# --- End of Inspection Logic ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d483875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Displaying 'Company Phone' for specific rows with Website = http://www.fit4future-foundation.ch ---\n",
      "                                  Company  \\\n",
      "39916  «fit4future»@fit4future foundation   \n",
      "\n",
      "                                   Website      Company Phone  \n",
      "39916  http://www.fit4future-foundation.ch  '+41 41 766 63 83  \n"
     ]
    }
   ],
   "source": [
    "# Display Company Phone Value of specific rows that match Website = http://www.fit4future-foundation.ch\n",
    "specific_rows = df_final_result_website_dedup[df_final_result_website_dedup['Website'] == 'http://www.fit4future-foundation.ch']\n",
    "print(\"\\n--- Displaying 'Company Phone' for specific rows with Website = http://www.fit4future-foundation.ch ---\")\n",
    "print(specific_rows[['Company', 'Website', 'Company Phone']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59c9263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count before 'Company' deduplication (with website preference): 84121\n",
      "Count after 'Company' deduplication (with website preference): 83884\n",
      "Number of rows dropped by this 'Company' deduplication step: 237\n",
      "Of the 237 rows dropped from non-empty company groups by the new logic:\n",
      "  - 237 had an empty Website.\n",
      "  - 0 had a non-empty Website (these would be from groups where all had empty websites, and these were not the 'first').\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # For robust string cleaning\n",
    "\"\"\"\n",
    "Deduplicates company records based on normalized company names and website presence.\n",
    "\n",
    "This block:\n",
    "- Normalizes company names to ensure consistent comparison (removes extra whitespace, handles NaN).\n",
    "- Separates rows with empty normalized company names for preservation.\n",
    "- For non-empty company names:\n",
    "    - Keeps unique company names as-is.\n",
    "    - For duplicated company names, keeps all rows with a website if any exist; otherwise, keeps only the first occurrence.\n",
    "- Combines all results into a final deduplicated DataFrame.\n",
    "- Reports the number of rows dropped and analyzes which dropped rows had empty or non-empty websites.\n",
    "- Intended to be run after website deduplication and before further processing.\n",
    "\n",
    "This logic helps ensure that each company is represented only once, with preference for records that include a website.\n",
    "\"\"\"\n",
    "\n",
    "df_current_processing = df_final_result_website_dedup.copy()\n",
    "\n",
    "# ...existing code...\n",
    "# Assume 'df_current_processing' is your DataFrame after the previous 'Website' deduplication.\n",
    "print(f\"Count before 'Company' deduplication (with website preference): {len(df_current_processing)}\")\n",
    "original_count_for_company_step = len(df_current_processing)\n",
    "\n",
    "\n",
    "\n",
    "# --- Step 1: Define a robust Company Name Normalization function ---\n",
    "def normalize_company_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\" # Represent NaN or None as an empty string for consistent handling\n",
    "    s = str(name)\n",
    "    s = re.sub(r'\\s+', ' ', s) # Replace multiple whitespace characters (including newlines, tabs) with a single space\n",
    "    s = s.strip() # Remove leading/trailing whitespace\n",
    "    return s\n",
    "\n",
    "# --- Step 2: Apply normalization and separate rows with empty normalized Company names ---\n",
    "# Create a temporary column for cleaned company names\n",
    "df_current_processing['__cleaned_company__'] = df_current_processing['Company'].apply(normalize_company_name)\n",
    "\n",
    "empty_company_mask = df_current_processing['__cleaned_company__'] == ''\n",
    "df_kept_empty_normalized_company = df_current_processing[empty_company_mask].copy() # Keep these rows\n",
    "df_to_process_non_empty_company = df_current_processing[~empty_company_mask].copy() # Process these\n",
    "\n",
    "# We can drop the temporary column from df_kept_empty_normalized_company now\n",
    "if '__cleaned_company__' in df_kept_empty_normalized_company.columns:\n",
    "    df_kept_empty_normalized_company.drop(columns=['__cleaned_company__'], inplace=True)\n",
    "\n",
    "# --- Step 3: Process rows with non-empty normalized 'Company' names ---\n",
    "df_kept_unique_companies = pd.DataFrame(columns=df_current_processing.columns).drop(columns=['__cleaned_company__'], errors='ignore')\n",
    "df_resolved_company_duplicates = pd.DataFrame(columns=df_current_processing.columns).drop(columns=['__cleaned_company__'], errors='ignore')\n",
    "\n",
    "\n",
    "if not df_to_process_non_empty_company.empty:\n",
    "    company_counts = df_to_process_non_empty_company['__cleaned_company__'].value_counts()\n",
    "\n",
    "    # 3a: Identify and keep companies that are already unique after normalization\n",
    "    unique_company_names = company_counts[company_counts == 1].index\n",
    "    df_kept_unique_companies_intermediate = df_to_process_non_empty_company[\n",
    "        df_to_process_non_empty_company['__cleaned_company__'].isin(unique_company_names)\n",
    "    ]\n",
    "    # Drop the temp column before assigning\n",
    "    if '__cleaned_company__' in df_kept_unique_companies_intermediate.columns:\n",
    "         df_kept_unique_companies = df_kept_unique_companies_intermediate.drop(columns=['__cleaned_company__'])\n",
    "    else:\n",
    "         df_kept_unique_companies = df_kept_unique_companies_intermediate\n",
    "\n",
    "\n",
    "    # 3b: Isolate groups of rows where the normalized 'Company' name is duplicated\n",
    "    duplicated_company_names = company_counts[company_counts > 1].index\n",
    "    df_groups_to_resolve = df_to_process_non_empty_company[\n",
    "        df_to_process_non_empty_company['__cleaned_company__'].isin(duplicated_company_names)\n",
    "    ]\n",
    "\n",
    "    def resolve_company_duplicates_by_website(group_df):\n",
    "        # Normalize 'Website' for checking emptiness (NaN, empty string, or string with only spaces)\n",
    "        has_website_mask = group_df['Website'].notna() & (group_df['Website'].astype(str).str.strip() != '')\n",
    "        \n",
    "        if has_website_mask.any():\n",
    "            # If any row in this company group has a website, keep all rows that have a website.\n",
    "            # Rows for this company that don't have a website will be dropped.\n",
    "            return group_df[has_website_mask]\n",
    "        else:\n",
    "            # If no row in this company group has a website, keep the first occurrence.\n",
    "            return group_df.head(1)\n",
    "\n",
    "    if not df_groups_to_resolve.empty:\n",
    "        df_resolved_company_duplicates_intermediate = df_groups_to_resolve.groupby(\n",
    "            '__cleaned_company__', group_keys=False\n",
    "        ).apply(resolve_company_duplicates_by_website, include_groups=False) # Added include_groups=False\n",
    "\n",
    "        # Drop the temp column before assigning\n",
    "        if '__cleaned_company__' in df_resolved_company_duplicates_intermediate.columns:\n",
    "            df_resolved_company_duplicates = df_resolved_company_duplicates_intermediate.drop(columns=['__cleaned_company__'])\n",
    "        else:\n",
    "            df_resolved_company_duplicates = df_resolved_company_duplicates_intermediate\n",
    "    else:\n",
    "        # Ensure df_resolved_company_duplicates has the correct columns (without __cleaned_company__)\n",
    "        # if df_groups_to_resolve was empty\n",
    "        cols = [col for col in df_to_process_non_empty_company.columns if col != '__cleaned_company__']\n",
    "        df_resolved_company_duplicates = pd.DataFrame(columns=cols)\n",
    "\n",
    "\n",
    "# --- Step 4: Combine all parts ---\n",
    "df_processed_step2_company_dedup = pd.concat([\n",
    "    df_kept_empty_normalized_company,\n",
    "    df_kept_unique_companies,\n",
    "    df_resolved_company_duplicates\n",
    "]).sort_index() # Restore original DataFrame order\n",
    "\n",
    "# Clean up __cleaned_company__ from df_current_processing if it's no longer needed globally\n",
    "if '__cleaned_company__' in df_current_processing.columns:\n",
    "    df_current_processing.drop(columns=['__cleaned_company__'], inplace=True)\n",
    "\n",
    "\n",
    "# --- Step 5: Output results ---\n",
    "# You can assign this result back to df_current_processing if needed for subsequent steps\n",
    "# df_current_processing = df_processed_step2_company_dedup\n",
    "\n",
    "final_count = len(df_processed_step2_company_dedup)\n",
    "rows_dropped_new_logic = original_count_for_company_step - final_count\n",
    "\n",
    "print(f\"Count after 'Company' deduplication (with website preference): {final_count}\")\n",
    "print(f\"Number of rows dropped by this 'Company' deduplication step: {rows_dropped_new_logic}\")\n",
    "\n",
    "# For a more direct comparison with your previous \"121\" observation:\n",
    "# This part is for analysis, not part of the core deduplication.\n",
    "# We need to re-evaluate what was dropped.\n",
    "# Let's identify rows in the original df_to_process_non_empty_company (before new dedup)\n",
    "# that were dropped by the new logic.\n",
    "\n",
    "# Create a set of indices from the original df_to_process_non_empty_company\n",
    "if not df_to_process_non_empty_company.empty:\n",
    "    original_indices_processed = set(df_to_process_non_empty_company.index)\n",
    "    \n",
    "    # Create a set of indices from the rows kept after processing non-empty companies\n",
    "    kept_indices_from_processing = set()\n",
    "    if not df_kept_unique_companies.empty:\n",
    "        kept_indices_from_processing.update(df_kept_unique_companies.index)\n",
    "    if not df_resolved_company_duplicates.empty:\n",
    "        kept_indices_from_processing.update(df_resolved_company_duplicates.index)\n",
    "        \n",
    "    dropped_indices_in_processing = original_indices_processed - kept_indices_from_processing\n",
    "    \n",
    "    if dropped_indices_in_processing:\n",
    "        df_actually_dropped_non_empty_company = df_to_process_non_empty_company.loc[list(dropped_indices_in_processing)]\n",
    "        \n",
    "        count_dropped_with_empty_website = (\n",
    "            df_actually_dropped_non_empty_company['Website'].isna() | \n",
    "            (df_actually_dropped_non_empty_company['Website'].astype(str).str.strip() == '')\n",
    "        ).sum()\n",
    "        print(f\"Of the {len(df_actually_dropped_non_empty_company)} rows dropped from non-empty company groups by the new logic:\")\n",
    "        print(f\"  - {count_dropped_with_empty_website} had an empty Website.\")\n",
    "        print(f\"  - {len(df_actually_dropped_non_empty_company) - count_dropped_with_empty_website} had a non-empty Website (these would be from groups where all had empty websites, and these were not the 'first').\")\n",
    "    else:\n",
    "        print(\"No rows were dropped from the non-empty company groups by the new logic.\")\n",
    "else:\n",
    "    print(\"No non-empty company rows to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62770580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original '# Employees' data type: float64\n",
      "Unique values sample: [27. 24. 29. 23. 18. 20. 19. 15. 16. 26. 28. 22. 17. 25. 21. 30. 14. 31.\n",
      " 33. 13.]\n",
      "Missing values in employees_numeric after coercion: 6\n",
      "Describe employees_numeric:\n",
      " count    83878.000000\n",
      "mean        38.895563\n",
      "std         28.397692\n",
      "min          1.000000\n",
      "25%         19.000000\n",
      "50%         28.000000\n",
      "75%         49.000000\n",
      "max        420.000000\n",
      "Name: employees_numeric, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "Converts the '# Employees' column to a numeric format for analysis.\n",
    "\n",
    "This block:\n",
    "- Prints the original data type and a sample of unique values from the '# Employees' column.\n",
    "- Converts the '# Employees' column to a numeric column 'employees_numeric', coercing errors to NaN.\n",
    "- Handles cases where values are not numbers (e.g., text or ranges like '10-50').\n",
    "- Prints the number of missing values after conversion and descriptive statistics for the new column.\n",
    "- Provides commented-out example code for handling ranges if needed.\n",
    "\n",
    "Intended for preparing employee count data for further analysis or binning.\n",
    "\"\"\"\n",
    " \n",
    "\n",
    "print(\"Original '# Employees' data type:\", df_processed_step2_company_dedup['# Employees'].dtype)\n",
    "print(\"Unique values sample:\", df_processed_step2_company_dedup['# Employees'].unique()[:20]) # See what kind of values exist\n",
    "\n",
    "# Convert to numeric, coercing errors to NaN (Not a Number)\n",
    "# This handles cases where some entries might be text like \"Not specified\" or ranges \"10-50\"\n",
    "df_processed_step2_company_dedup['employees_numeric'] = pd.to_numeric(df_processed_step2_company_dedup['# Employees'], errors='coerce')\n",
    "# For ranges (e.g., \"10-50\"), you might need more sophisticated parsing.\n",
    "# For now, to_numeric with 'coerce' is a good start. If you have many ranges,\n",
    "# you might need a custom function to extract the lower or average value.\n",
    "# Example for a specific string if '10-50' was common and became NaN:\n",
    "# def parse_employee_range(value):\n",
    "#     if isinstance(value, str) and '-' in value:\n",
    "#         try:\n",
    "#             low, high = map(int, value.split('-'))\n",
    "#             return (low + high) / 2 # Or just low\n",
    "#         except ValueError:\n",
    "#             return np.nan\n",
    "#     return value\n",
    "# df_processed_step2_company_dedup['# Employees'] = df_processed_step2_company_dedup['# Employees'].apply(parse_employee_range)\n",
    "# df_processed_step2_company_dedup['employees_numeric'] = pd.to_numeric(df_processed_step2_company_dedup['# Employees'], errors='coerce')\n",
    "print(\"Missing values in employees_numeric after coercion:\", df_processed_step2_company_dedup['employees_numeric'].isnull().sum())\n",
    "print(\"Describe employees_numeric:\\n\", df_processed_step2_company_dedup['employees_numeric'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5750b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Company_Size_Category:\n",
      " Company_Size_Category\n",
      "11-50                    65100\n",
      "51-250                   18748\n",
      "1-10                        27\n",
      "Unknown/Not Specified        6\n",
      "251-1000                     3\n",
      "1001+                        0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def categorize_company_size(df_processed_step2_company_dedup):\n",
    "    \"\"\"\n",
    "    Categorizes companies into size bins based on the 'employees_numeric' column.\n",
    "\n",
    "    - Defines bins and labels for company size categories.\n",
    "    - Uses pd.cut to assign each company a size category.\n",
    "    - Adds 'Unknown/Not Specified' as a category for missing or unclassified values.\n",
    "    - Fills missing values in the 'Company_Size_Category' column with 'Unknown/Not Specified'.\n",
    "    - Prints the value counts for each category.\n",
    "\n",
    "    Args:\n",
    "        df_processed_step2_company_dedup (pd.DataFrame): DataFrame containing an 'employees_numeric' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with a new 'Company_Size_Category' column.\n",
    "    \"\"\"\n",
    "    # Define bins and labels\n",
    "    employee_bins = [0, 10, 50, 250, 1000, np.inf]\n",
    "    employee_labels = ['1-10', '11-50', '51-250', '251-1000', '1001+']\n",
    "\n",
    "    # Create the 'Company_Size_Category' column using pd.cut\n",
    "    df_processed_step2_company_dedup['Company_Size_Category'] = pd.cut(\n",
    "        df_processed_step2_company_dedup['employees_numeric'],\n",
    "        bins=employee_bins,\n",
    "        labels=employee_labels,\n",
    "        right=True,\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    # Ensure 'Unknown/Not Specified' is a category and fill missing values\n",
    "    if isinstance(df_processed_step2_company_dedup['Company_Size_Category'].dtype, pd.CategoricalDtype):\n",
    "        if 'Unknown/Not Specified' not in df_processed_step2_company_dedup['Company_Size_Category'].cat.categories:\n",
    "            df_processed_step2_company_dedup['Company_Size_Category'] = df_processed_step2_company_dedup['Company_Size_Category'].cat.add_categories(['Unknown/Not Specified'])\n",
    "    df_processed_step2_company_dedup['Company_Size_Category'] = df_processed_step2_company_dedup['Company_Size_Category'].fillna('Unknown/Not Specified')\n",
    "\n",
    "    print(\"Value counts for Company_Size_Category:\\n\", df_processed_step2_company_dedup['Company_Size_Category'].value_counts(dropna=False))\n",
    "    return df_processed_step2_company_dedup\n",
    "\n",
    "# Usage:\n",
    "df_processed_step3_categorization = categorize_company_size(df_processed_step2_company_dedup.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65304684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5c: Inspecting and Cleaning 'Annual Revenue' ---\n",
      "Original 'Annual Revenue' data type: float64\n",
      "Sample unique 'Annual Revenue' values (first 50):\n",
      "['nan' '6329000.0' '3569000.0' '28982000.0' '234000.0' '479000.0'\n",
      " '2771000.0' '3172000.0' '201000.0' '349000.0' '25000000.0' '164000.0'\n",
      " '5867000.0' '6800000.0' '75000000.0' '2500000.0' '28144000.0'\n",
      " '28200000.0' '1700000.0' '9859000.0' '300000.0' '5000000.0' '398000.0'\n",
      " '10000000.0' '2000000.0' '1000000.0' '211000.0' '2600000.0' '546000.0'\n",
      " '17500000.0' '13832000.0' '7750000.0' '1500000.0' '1200000.0' '4504000.0'\n",
      " '13896000.0' '1773000.0' '14721000.0' '4893000.0' '5920000.0' '7497000.0'\n",
      " '35000000.0' '379000.0' '45000000.0' '750000000.0' '26526000.0'\n",
      " '5275000.0' '5937000.0' '148000.0' '26113000.0']\n",
      "Number of missing 'Annual Revenue' values: 72720\n",
      "\n",
      "Missing values in 'revenue_numeric' after parsing: 72720\n",
      "Descriptive statistics for 'revenue_numeric':\n",
      "count    1.116400e+04\n",
      "mean     1.994372e+08\n",
      "std      7.414902e+09\n",
      "min      1.000000e+03\n",
      "25%      2.200000e+06\n",
      "50%      6.100000e+06\n",
      "75%      1.829700e+07\n",
      "max      5.452210e+11\n",
      "Name: revenue_numeric, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # For more advanced string cleaning\n",
    "\n",
    "print(\"\\n--- Step 5c: Inspecting and Cleaning 'Annual Revenue' ---\")\n",
    "\n",
    "# Ensure 'Annual Revenue' column exists\n",
    "if 'Annual Revenue' not in df_processed_step3_categorization.columns:\n",
    "    print(\"'Annual Revenue' column not found. Skipping this step.\")\n",
    "else:\n",
    "    print(\"Original 'Annual Revenue' data type:\", df_processed_step3_categorization['Annual Revenue'].dtype)\n",
    "    print(\"Sample unique 'Annual Revenue' values (first 50):\")\n",
    "    # Print unique values, converting to string to handle mixed types and NaNs gracefully for display\n",
    "    unique_revenues = df_processed_step3_categorization['Annual Revenue'].astype(str).unique()\n",
    "    print(unique_revenues[:50])\n",
    "    print(f\"Number of missing 'Annual Revenue' values: {df_processed_step3_categorization['Annual Revenue'].isnull().sum()}\")\n",
    "\n",
    "    # --- Robust Revenue Parsing Function ---\n",
    "    def parse_revenue(value):\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        s = str(value).lower().strip()\n",
    "        \n",
    "        # Remove currency symbols and commas\n",
    "        s = re.sub(r'[$,€£¥]', '', s)\n",
    "        s = s.replace(',', '')\n",
    "\n",
    "        # Handle 'k' for thousands, 'm' for millions, 'b' for billions\n",
    "        multiplier = 1\n",
    "        if 'b' in s or 'billion' in s:\n",
    "            multiplier = 1_000_000_000\n",
    "            s = re.sub(r'[b\\s]*illion', '', s, flags=re.IGNORECASE) # Remove 'billion' and 'b'\n",
    "            s = s.replace('b', '') # Ensure standalone 'b' is also removed\n",
    "        elif 'm' in s or 'million' in s:\n",
    "            multiplier = 1_000_000\n",
    "            s = re.sub(r'[m\\s]*illion', '', s, flags=re.IGNORECASE) # Remove 'million' and 'm'\n",
    "            s = s.replace('m', '') # Ensure standalone 'm' is also removed\n",
    "        elif 'k' in s or 'thousand' in s:\n",
    "            multiplier = 1_000\n",
    "            s = re.sub(r'[k\\s]*thousand', '', s, flags=re.IGNORECASE) # Remove 'thousand' and 'k'\n",
    "            s = s.replace('k', '') # Ensure standalone 'k' is also removed\n",
    "\n",
    "        # Handle ranges (e.g., \"10-20\", \"5 to 10\") - take the lower bound or average\n",
    "        range_match = re.match(r'([\\d\\.]+)\\s*(?:to|-)\\s*([\\d\\.]+)', s)\n",
    "        if range_match:\n",
    "            try:\n",
    "                low = float(range_match.group(1))\n",
    "                # high = float(range_match.group(2)) # If you want to use the high or average\n",
    "                s = str(low) # Using the lower bound\n",
    "            except ValueError:\n",
    "                return np.nan # If parts of range are not numbers\n",
    "\n",
    "        # Try to convert to float\n",
    "        try:\n",
    "            numeric_val = float(s)\n",
    "            return numeric_val * multiplier\n",
    "        except ValueError:\n",
    "            return np.nan # Could not convert to number\n",
    "\n",
    "    # Apply the parsing function\n",
    "    df_processed_step3_categorization['revenue_numeric'] = df_processed_step3_categorization['Annual Revenue'].apply(parse_revenue)\n",
    "\n",
    "    print(f\"\\nMissing values in 'revenue_numeric' after parsing: {df_processed_step3_categorization['revenue_numeric'].isnull().sum()}\")\n",
    "    print(\"Descriptive statistics for 'revenue_numeric':\")\n",
    "    print(df_processed_step3_categorization['revenue_numeric'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1023ec0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique industries in dataset: 145\n",
      "['events services' 'architecture & planning' 'accounting'\n",
      " 'staffing & recruiting' 'information technology & services'\n",
      " 'nonprofit organization management' 'food & beverages' 'insurance'\n",
      " 'marketing & advertising' 'hospitality' 'political organization'\n",
      " 'electrical/electronic manufacturing' 'mining & metals'\n",
      " 'health, wellness & fitness' 'transportation/trucking/railroad'\n",
      " 'entertainment' 'logistics & supply chain' 'machinery'\n",
      " 'building materials' 'recreational facilities & services'\n",
      " 'financial services' 'biotechnology' 'performing arts' 'food production'\n",
      " 'government administration' 'luxury goods & jewelry' 'construction'\n",
      " 'plastics' 'design' 'legal services' 'public relations & communications'\n",
      " 'media production' 'business supplies & equipment'\n",
      " 'environmental services' 'international trade & development'\n",
      " 'education management' 'human resources' 'civil engineering' 'retail'\n",
      " 'leisure, travel & tourism' 'automotive' 'wholesale' 'medical practice'\n",
      " 'telecommunications' 'market research' 'wine & spirits'\n",
      " 'professional training & coaching' 'furniture' 'nan'\n",
      " 'facilities services' 'utilities' 'consumer services' 'research'\n",
      " 'higher education' 'computer & network security' 'real estate'\n",
      " 'management consulting' 'pharmaceuticals' 'veterinary'\n",
      " 'computer hardware' 'apparel & fashion' 'oil & energy'\n",
      " 'civic & social organization' 'writing & editing' 'medical devices'\n",
      " 'renewables & environment' 'mechanical or industrial engineering'\n",
      " 'information services' 'semiconductors' 'import & export' 'restaurants'\n",
      " 'international affairs' 'computer networking' 'sports' 'publishing'\n",
      " 'translation & localization' 'cosmetics' 'aviation & aerospace'\n",
      " 'printing' 'e-learning' 'law practice' 'nanotechnology' 'sporting goods'\n",
      " 'museums & institutions' 'online media' 'chemicals' 'public policy'\n",
      " 'hospital & health care' 'primary/secondary education' 'music'\n",
      " 'security & investigations' 'textiles' 'individual & family services'\n",
      " 'banking' 'airlines/aviation' 'religious institutions'\n",
      " 'venture capital & private equity' 'mental health care'\n",
      " 'railroad manufacture' 'graphic design' 'farming' 'alternative medicine'\n",
      " 'packaging & containers' 'glass, ceramics & concrete' 'fine art'\n",
      " 'package/freight delivery' 'investment management' 'defense & space'\n",
      " 'paper & forest products' 'warehousing' 'computer software' 'maritime'\n",
      " 'think tanks' 'law enforcement' 'computer games' 'photography' 'internet'\n",
      " 'shipbuilding' 'public safety' 'executive office' 'consumer goods'\n",
      " 'industrial automation' 'investment banking'\n",
      " 'alternative dispute resolution' 'dairy' 'commercial real estate'\n",
      " 'government relations' 'outsourcing/offshoring' 'gambling & casinos'\n",
      " 'libraries' 'capital markets' 'broadcast media' 'military' 'animation'\n",
      " 'philanthropy' 'arts & crafts' 'consumer electronics' 'fund-raising'\n",
      " 'wireless' 'tobacco' 'program development' 'fishery' 'legislative office'\n",
      " 'newspapers' 'ranching' 'agriculture']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique industries in dataset: {df_processed_step3_categorization['Industry'].nunique()}\")\n",
    "unique_industries_large = df_processed_step3_categorization['Industry'].astype(str).unique()\n",
    "print(unique_industries_large[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb91ed8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6: Process Industry and SIC Codes ---\n",
      "\n",
      "--- Step 6a: Inspecting the Existing 'Industry' Column ---\n",
      "Number of unique industries in dataset: 145\n",
      "Sample unique industries (first 50):\n",
      "['events services' 'architecture & planning' 'accounting'\n",
      " 'staffing & recruiting' 'information technology & services'\n",
      " 'nonprofit organization management' 'food & beverages' 'insurance'\n",
      " 'marketing & advertising' 'hospitality' 'political organization'\n",
      " 'electrical/electronic manufacturing' 'mining & metals'\n",
      " 'health, wellness & fitness' 'transportation/trucking/railroad'\n",
      " 'entertainment' 'logistics & supply chain' 'machinery'\n",
      " 'building materials' 'recreational facilities & services'\n",
      " 'financial services' 'biotechnology' 'performing arts' 'food production'\n",
      " 'government administration' 'luxury goods & jewelry' 'construction'\n",
      " 'plastics' 'design' 'legal services' 'public relations & communications'\n",
      " 'media production' 'business supplies & equipment'\n",
      " 'environmental services' 'international trade & development'\n",
      " 'education management' 'human resources' 'civil engineering' 'retail'\n",
      " 'leisure, travel & tourism' 'automotive' 'wholesale' 'medical practice'\n",
      " 'telecommunications' 'market research' 'wine & spirits'\n",
      " 'professional training & coaching' 'furniture' 'nan'\n",
      " 'facilities services']\n",
      "\n",
      "Value counts for 'Industry' (Top 30):\n",
      "Industry\n",
      "information technology & services       10150\n",
      "NaN                                      4597\n",
      "machinery                                4414\n",
      "construction                             2528\n",
      "marketing & advertising                  2351\n",
      "management consulting                    2349\n",
      "government administration                2185\n",
      "hospital & health care                   1899\n",
      "automotive                               1821\n",
      "financial services                       1817\n",
      "research                                 1544\n",
      "electrical/electronic manufacturing      1533\n",
      "retail                                   1458\n",
      "real estate                              1403\n",
      "wholesale                                1357\n",
      "hospitality                              1342\n",
      "health, wellness & fitness               1299\n",
      "architecture & planning                  1247\n",
      "civic & social organization              1089\n",
      "medical devices                          1038\n",
      "higher education                         1037\n",
      "media production                         1004\n",
      "food production                           988\n",
      "staffing & recruiting                     976\n",
      "logistics & supply chain                  922\n",
      "banking                                   908\n",
      "nonprofit organization management         887\n",
      "professional training & coaching          885\n",
      "mechanical or industrial engineering      795\n",
      "utilities                                 784\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Step 6b: Create the Mapping Dictionary (Manual Task) ---\n",
      "Initial mapping dictionary created with 146 entries.\n",
      "Remember to complete this dictionary based on the unique industries printed above.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 6: Process Industry and SIC Codes ---\")\n",
    "print(\"\\n--- Step 6a: Inspecting the Existing 'Industry' Column ---\")\n",
    "\n",
    "if 'Industry' not in df_processed_step3_categorization.columns:\n",
    "    print(\"'Industry' column not found. Skipping Step 6.\")\n",
    "else:\n",
    "    print(f\"Number of unique industries in dataset: {df_processed_step3_categorization['Industry'].nunique()}\")\n",
    "    print(\"Sample unique industries (first 50):\")\n",
    "    # Print unique values, converting to string to handle mixed types and NaNs gracefully for display\n",
    "    unique_industries_large = df_processed_step3_categorization['Industry'].astype(str).unique()\n",
    "    print(unique_industries_large[:50]) # Shows a sample\n",
    "    # For a full list to help with mapping, you might want to print all:\n",
    "    # print(\"\\nFull list of unique industries:\")\n",
    "    # for industry in sorted(list(unique_industries_large)): # Sorted for easier review\n",
    "    #     print(f\"'{industry}',\")\n",
    "\n",
    "    print(\"\\nValue counts for 'Industry' (Top 30):\")\n",
    "    print(df_processed_step3_categorization['Industry'].value_counts(dropna=False).nlargest(30))\n",
    "\n",
    "    print(\"\\n--- Step 6b: Create the Mapping Dictionary (Manual Task) ---\")\n",
    "    # This is where you'll MANUALLY build your mapping dictionary based on the output from 6a\n",
    "    # and your standardized categories from the 70 partner companies.\n",
    "    # Your standardized categories (examples, ensure these match your actual list):\n",
    "    # \"Business Consulting / Management Consulting\", \"Healthcare Technology / HealthTech\",\n",
    "    # \"Cybersecurity Services\", \"HR / Recruitment\", \"Software Development / SaaS\",\n",
    "    # \"Digital Marketing Agency / Web Development\", \"Logistics Technology / Supply Chain Tech\", etc.\n",
    "\n",
    "    industry_mapping_large_to_standard = {\n",
    "    'events services': 'Business Consulting / Management Consulting', # Or 'Service-Oriented (General B2B)'\n",
    "    'architecture & planning': 'Business Consulting / Management Consulting', # Or a new \"Architecture / Planning\" if needed\n",
    "    'accounting': 'Financial Services / Consulting', # (Accounting Sub-sector)\n",
    "    'staffing & recruiting': 'HR / Recruitment',\n",
    "    'information technology & services': 'IT Services / Managed Services',\n",
    "    'nonprofit organization management': 'Non-Profits / Associations (B2B)',\n",
    "    'food & beverages': 'Food & Beverage',\n",
    "    'insurance': 'Financial Services / Consulting', # (Insurance Sub-sector)\n",
    "    'marketing & advertising': 'Digital Marketing Agency / Web Development',\n",
    "    'hospitality': 'Retail Technology / Hospitality Technology', # Or a broader \"Hospitality Sector (B2B)\"\n",
    "    'political organization': 'Non-Profits / Associations (B2B)', # Or 'Other'\n",
    "    'electrical/electronic manufacturing': 'Manufacturing Sector (B2B)',\n",
    "    'mining & metals': 'Manufacturing Sector (B2B)', # Or 'Other' if not a fit\n",
    "    'health, wellness & fitness': 'Healthcare Services', # Or 'Wellness Technology / Alternative Health Products'\n",
    "    'transportation/trucking/railroad': 'Logistics Technology / Supply Chain Tech',\n",
    "    'entertainment': 'Digital Media / Creative Tech', # Or 'Service-Oriented (General B2B)'\n",
    "    'logistics & supply chain': 'Logistics Technology / Supply Chain Tech',\n",
    "    'machinery': 'Manufacturing Sector (B2B)',\n",
    "    'building materials': 'Wholesale / Distribution', # Or 'Manufacturing Sector (B2B)'\n",
    "    'recreational facilities & services': 'Retail Technology / Hospitality Technology', # Or 'Service-Oriented'\n",
    "    'financial services': 'Financial Services / Consulting',\n",
    "    'biotechnology': 'Healthcare Technology / HealthTech', # Or specific 'Biotechnology' if needed\n",
    "    'performing arts': 'Digital Media / Creative Tech', # Or 'Service-Oriented'\n",
    "    'food production': 'Food & Beverage',\n",
    "    'government administration': 'Public Sector / Government (B2B)',\n",
    "    'luxury goods & jewelry': 'Retail Sector (B2B)',\n",
    "    'construction': 'Manufacturing Sector (B2B)', # Or \"Real Estate / Construction\" if you add it\n",
    "    'plastics': 'Manufacturing Sector (B2B)',\n",
    "    'design': 'Digital Media / Creative Tech', # Or 'Business Consulting / Management Consulting'\n",
    "    'legal services': 'Legal Technology', # Or 'Service-Oriented (Legal)'\n",
    "    'public relations & communications': 'Digital Marketing Agency / Web Development',\n",
    "    'media production': 'Digital Media / Creative Tech',\n",
    "    'business supplies & equipment': 'Wholesale / Distribution',\n",
    "    'environmental services': 'Green Technology / Sustainability Services',\n",
    "    'international trade & development': 'Business Consulting / Management Consulting', # Or 'Wholesale / Distribution'\n",
    "    'education management': 'EdTech / E-Learning', # Or 'Education Sector (B2B)'\n",
    "    'human resources': 'HR / Recruitment',\n",
    "    'civil engineering': 'Business Consulting / Management Consulting', # Or \"Real Estate / Construction\"\n",
    "    'retail': 'Retail Sector (B2B)',\n",
    "    'leisure, travel & tourism': 'Retail Technology / Hospitality Technology', # Or 'Service-Oriented'\n",
    "    'automotive': 'Manufacturing Sector (B2B)', # (Automotive Sub-sector)\n",
    "    'wholesale': 'Wholesale / Distribution',\n",
    "    'medical practice': 'Healthcare Services',\n",
    "    'telecommunications': 'Telecommunications Services / Infrastructure',\n",
    "    'market research': 'Business Consulting / Management Consulting', # Or 'Digital Marketing Agency'\n",
    "    'wine & spirits': 'Food & Beverage', # Or 'Wholesale / Distribution' / 'Retail'\n",
    "    'professional training & coaching': 'Business Consulting / Management Consulting', # Or 'EdTech / E-Learning'\n",
    "    'furniture': 'Retail Sector (B2B)', # Or 'Manufacturing Sector (B2B)'\n",
    "    'nan': 'Unknown/Not Specified', # Map the string 'nan' to your unknown category or np.nan\n",
    "    'facilities services': 'Service-Oriented (General B2B)', # Or 'Business Consulting'\n",
    "    'utilities': 'Green Technology / Sustainability Services', # Or 'Energy Sector (B2B)' if you add it\n",
    "    'consumer services': 'Service-Oriented (General B2B)', # Very broad\n",
    "    'research': 'Business Consulting / Management Consulting', # Or 'Other', depends on type of research\n",
    "    'higher education': 'EdTech / E-Learning', # Or 'Education Sector (B2B)'\n",
    "    'computer & network security': 'Cybersecurity Services',\n",
    "    'real estate': 'Financial Services / Consulting', # Or a new \"Real Estate Services\"\n",
    "    'management consulting': 'Business Consulting / Management Consulting',\n",
    "    'pharmaceuticals': 'Pharmaceutical Services / Consulting', # Or 'Healthcare Technology / HealthTech'\n",
    "    'veterinary': 'Healthcare Services',\n",
    "    'computer hardware': 'IT Services / Managed Services', # Or 'Product-Driven (Physical)'\n",
    "    'apparel & fashion': 'Retail Sector (B2B)',\n",
    "    'oil & energy': 'Green Technology / Sustainability Services', # Or 'Energy Sector (B2B)'\n",
    "    'civic & social organization': 'Non-Profits / Associations (B2B)',\n",
    "    'writing & editing': 'Digital Media / Creative Tech', # Or 'Service-Oriented'\n",
    "    'medical devices': 'Healthcare Technology / HealthTech', # (MedTech sub-focus)\n",
    "    'renewables & environment': 'Green Technology / Sustainability Services',\n",
    "    'mechanical or industrial engineering': 'Manufacturing Sector (B2B)', # Or 'Business Consulting'\n",
    "    'information services': 'IT Services / Managed Services', # Broad, could also be 'Software Dev'\n",
    "    'semiconductors': 'Manufacturing Sector (B2B)', # (Electronics sub-focus)\n",
    "    'import & export': 'Wholesale / Distribution',\n",
    "    'restaurants': 'Retail Technology / Hospitality Technology', # Or 'Food & Beverage'\n",
    "    'international affairs': 'Business Consulting / Management Consulting', # Or 'Other'\n",
    "    'computer networking': 'IT Services / Managed Services', # Or 'Telecommunications'\n",
    "    'sports': 'Digital Media / Creative Tech', # Or 'Service-Oriented'\n",
    "    'publishing': 'Digital Media / Creative Tech', # Or specific \"Publishing\" category\n",
    "    'translation & localization': 'Service-Oriented (General B2B)', # Or a new \"Translation Services\"\n",
    "    'cosmetics': 'Retail Sector (B2B)',\n",
    "    'aviation & aerospace': 'Aerospace Technology / Space Tech',\n",
    "    'printing': 'Manufacturing Services / PaaS', # Or 'Service-Oriented (General B2B)'\n",
    "    'e-learning': 'EdTech / E-Learning',\n",
    "    'law practice': 'Legal Technology', # Or 'Service-Oriented (Legal)'\n",
    "    'nanotechnology': 'Other', # Or 'Software Development / SaaS' if tech-focused\n",
    "    'sporting goods': 'Retail Sector (B2B)',\n",
    "    'museums & institutions': 'Non-Profits / Associations (B2B)', # Or 'Education Sector'\n",
    "    'online media': 'Digital Media / Creative Tech',\n",
    "    'chemicals': 'Manufacturing Sector (B2B)', # (Chemicals sub-focus)\n",
    "    'public policy': 'Business Consulting / Management Consulting', # Or 'Public Sector'\n",
    "    'hospital & health care': 'Healthcare Services', # Or 'Healthcare Technology / HealthTech' depending on company\n",
    "    'primary/secondary education': 'EdTech / E-Learning', # Or 'Education Sector (B2B)'\n",
    "    'music': 'Digital Media / Creative Tech',\n",
    "    'security & investigations': 'Cybersecurity Services', # Or 'Security Technology (Physical)'\n",
    "    'textiles': 'Manufacturing Sector (B2B)',\n",
    "    'individual & family services': 'Service-Oriented (General B2B)', # Broad\n",
    "    'banking': 'Financial Services / Consulting',\n",
    "    'airlines/aviation': 'Aerospace Technology / Space Tech',\n",
    "    'religious institutions': 'Non-Profits / Associations (B2B)',\n",
    "    'venture capital & private equity': 'Financial Services / Consulting',\n",
    "    'mental health care': 'Healthcare Services',\n",
    "    'railroad manufacture': 'Manufacturing Sector (B2B)',\n",
    "    'graphic design': 'Digital Media / Creative Tech',\n",
    "    'farming': 'Food & Beverage', # Or a new 'Agriculture'\n",
    "    'alternative medicine': 'Wellness Technology / Alternative Health Products', # Or 'Healthcare Services'\n",
    "    'packaging & containers': 'Packaging Solutions',\n",
    "    'glass, ceramics & concrete': 'Manufacturing Sector (B2B)',\n",
    "    'fine art': 'Digital Media / Creative Tech', # Or 'Retail Sector (B2B)'\n",
    "    'package/freight delivery': 'Logistics Technology / Supply Chain Tech',\n",
    "    'investment management': 'Financial Services / Consulting',\n",
    "    'defense & space': 'Aerospace Technology / Space Tech', # Or specific \"Defense\" category\n",
    "    'paper & forest products': 'Manufacturing Sector (B2B)',\n",
    "    'warehousing': 'Logistics Technology / Supply Chain Tech',\n",
    "    'computer software': 'Software Development / SaaS',\n",
    "    'maritime': 'Logistics Technology / Supply Chain Tech',\n",
    "    'think tanks': 'Business Consulting / Management Consulting', # Or 'Other'\n",
    "    'law enforcement': 'Public Sector / Government (B2B)',\n",
    "    'computer games': 'Software Development / SaaS', # (Gaming focus) or 'Digital Media'\n",
    "    'photography': 'Digital Media / Creative Tech', # Or 'Service-Oriented'\n",
    "    'internet': 'IT Services / Managed Services', # Could also be 'Software Development / SaaS'\n",
    "    'shipbuilding': 'Manufacturing Sector (B2B)',\n",
    "    'public safety': 'Public Sector / Government (B2B)', # Or 'Security Tech'\n",
    "    'executive office': 'Business Consulting / Management Consulting', # Or 'Public Sector'\n",
    "    'consumer goods': 'Wholesale / Distribution', # Or 'Retail Sector (B2B)'\n",
    "    'industrial automation': 'Automation Technology',\n",
    "    'investment banking': 'Financial Services / Consulting',\n",
    "    'alternative dispute resolution': 'Legal Technology', # Or 'Service-Oriented (Legal)'\n",
    "    'dairy': 'Food & Beverage',\n",
    "    'commercial real estate': 'Financial Services / Consulting', # Or \"Real Estate Services\"\n",
    "    'government relations': 'Business Consulting / Management Consulting', # Or 'Public Sector'\n",
    "    'outsourcing/offshoring': 'Business Consulting / Management Consulting', # Or 'IT Services'\n",
    "    'gambling & casinos': 'Retail Technology / Hospitality Technology', # Or 'Other'\n",
    "    'libraries': 'Public Sector / Government (B2B)', # Or 'Education Sector'\n",
    "    'capital markets': 'Financial Services / Consulting',\n",
    "    'broadcast media': 'Digital Media / Creative Tech',\n",
    "    'military': 'Public Sector / Government (B2B)', # Or specific \"Defense\"\n",
    "    'animation': 'Digital Media / Creative Tech',\n",
    "    'philanthropy': 'Non-Profits / Associations (B2B)',\n",
    "    'arts & crafts': 'Retail Sector (B2B)',\n",
    "    'consumer electronics': 'Retail Sector (B2B)', # Or 'Manufacturing Sector (B2B)'\n",
    "    'fund-raising': 'Non-Profits / Associations (B2B)',\n",
    "    'wireless': 'Telecommunications Services / Infrastructure',\n",
    "    'tobacco': 'Wholesale / Distribution', # Or 'Other'\n",
    "    'program development': 'Software Development / SaaS', # Or 'Business Consulting'\n",
    "    'fishery': 'Food & Beverage', # Or 'Other'\n",
    "    'legislative office': 'Public Sector / Government (B2B)',\n",
    "    'newspapers': 'Digital Media / Creative Tech', # Or \"Publishing\"\n",
    "    'ranching': 'Food & Beverage', # Or 'Other' / 'Agriculture'\n",
    "    'agriculture': 'Food & Beverage', # Or a new 'Agriculture'\n",
    "    }\n",
    "    print(f\"Initial mapping dictionary created with {len(industry_mapping_large_to_standard)} entries.\")\n",
    "    print(\"Remember to complete this dictionary based on the unique industries printed above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59392797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6c: Applying the Industry Mapping ---\n",
      "Number of rows where 'Industry_Category_Standardized' is NaN: 4597\n",
      "Breakdown of NaNs in 'Industry_Category_Standardized':\n",
      "  - Caused by original NaNs in 'Industry' column: 4597\n",
      "  - Caused by 'Industry' values not found in mapping dictionary: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 6c (Apply Mapping - run this AFTER you've finalized the dictionary in 6b)\n",
    "print(\"\\n--- Step 6c: Applying the Industry Mapping ---\")\n",
    "\n",
    "# Ensure the column name 'Industry' matches your DataFrame\n",
    "if 'Industry' in df_processed_step3_categorization.columns:\n",
    "    df_processed_step3_categorization['Industry_Category_Standardized'] = df_processed_step3_categorization['Industry'].map(industry_mapping_large_to_standard)\n",
    "\n",
    "    unmapped_count = df_processed_step3_categorization['Industry_Category_Standardized'].isnull().sum()\n",
    "    # This count includes original NaNs in 'Industry' plus any keys not found in your map\n",
    "    print(f\"Number of rows where 'Industry_Category_Standardized' is NaN: {unmapped_count}\")\n",
    "\n",
    "    if unmapped_count > 0:\n",
    "        print(\"Breakdown of NaNs in 'Industry_Category_Standardized':\")\n",
    "        original_nans_in_industry = df_processed_step3_categorization['Industry'].isnull().sum()\n",
    "        print(f\"  - Caused by original NaNs in 'Industry' column: {original_nans_in_industry}\")\n",
    "        \n",
    "        # Identify industries that were present in data but NOT in mapping dictionary (excluding original NaNs)\n",
    "        unmapped_industries_from_data = df_processed_step3_categorization[\n",
    "            df_processed_step3_categorization['Industry'].notna() & df_processed_step3_categorization['Industry_Category_Standardized'].isnull()\n",
    "        ]['Industry'].astype(str).unique()\n",
    "        \n",
    "        num_truly_unmapped = len(unmapped_industries_from_data)\n",
    "        print(f\"  - Caused by 'Industry' values not found in mapping dictionary: {num_truly_unmapped}\")\n",
    "\n",
    "        if num_truly_unmapped > 0:\n",
    "            print(\"  Industries present in data but MISSING from mapping dictionary (sample of up to 50):\")\n",
    "            print(unmapped_industries_from_data[:50])\n",
    "            print(\"  ACTION: Add these to 'industry_mapping_large_to_standard' in Step 6b and re-run 6b and this 6c cell.\")\n",
    "else:\n",
    "    print(\"'Industry' column not found in df_processed_step3_categorization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddda80d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records that have SIC codes AND 'Industry_Category_Standardized' is still NaN: 146\n",
      "Unique SIC codes among these records (sample):\n",
      "['1000', '1000, 1090, 3317, 1220, 1000, 3334', '1382', '1382, 1382, 4900, 1623', '1731, 3317', '2030', '2300', '2741', '2836, 2836', '3317', '3537', '3537, 5531', '3630', '3821, 3812', '4581', '4700', '4833', '5000', '5047', '5331, 5990', '5990', '6199', '6199, 6199', '6199, 6411, 6199', '6282', '6500', '7011', '7311', '7371', '7371, 7372', '7372', '7373', '7380', '7900', '7900, 7011', '8000', '8000, 5047', '8000, 8060', '8060', '8200', '8200, 8200', '8200, 8200, 8200', '8300', '8600', '8731, 2836', '8742']\n"
     ]
    }
   ],
   "source": [
    "# After Step 6c (primary industry mapping) and before Step 6d\n",
    "if 'SIC Codes' in df_processed_step3_categorization.columns and 'Industry_Category_Standardized' in df_processed_step3_categorization.columns:\n",
    "    sic_present_mask = df_processed_step3_categorization['SIC Codes'].notna()\n",
    "    industry_still_nan_mask = df_processed_step3_categorization['Industry_Category_Standardized'].isnull()\n",
    "    \n",
    "    records_with_sic_and_nan_industry = df_processed_step3_categorization[sic_present_mask & industry_still_nan_mask]\n",
    "    count_can_potentially_fill_via_sic = len(records_with_sic_and_nan_industry)\n",
    "    \n",
    "    print(f\"Number of records that have SIC codes AND 'Industry_Category_Standardized' is still NaN: {count_can_potentially_fill_via_sic}\")\n",
    "\n",
    "    if count_can_potentially_fill_via_sic > 0:\n",
    "        print(\"Unique SIC codes among these records (sample):\")\n",
    "        # Helper to get unique first SIC codes from this subset\n",
    "        def get_first_sic(sic_val):\n",
    "            if pd.isna(sic_val): return None\n",
    "            return str(sic_val).split(';')[0].strip()\n",
    "        \n",
    "        unique_sics_to_potentially_map = records_with_sic_and_nan_industry['SIC Codes'].apply(get_first_sic).dropna().unique()\n",
    "        print(sorted(list(unique_sics_to_potentially_map))[:50]) # Print a sample of SICs you might target\n",
    "    else:\n",
    "        print(\"No records with SIC codes need their industry category filled (already filled by primary industry mapping).\")\n",
    "else:\n",
    "    print(\"Required columns ('SIC Codes' or 'Industry_Category_Standardized') not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2720ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6d: Filling Remaining NaNs in Industry_Category_Standardized ---\n",
      "Found 4597 NaN values in 'Industry_Category_Standardized' before final fill (these include the 148 that had SICs).\n",
      "Filled remaining NaNs in 'Industry_Category_Standardized' with 'Unknown/Not Specified'.\n",
      "\n",
      "Value counts for 'Industry_Category_Standardized' (final):\n",
      "Industry_Category_Standardized\n",
      "Manufacturing Sector (B2B)                           14439\n",
      "IT Services / Managed Services                       10536\n",
      "Business Consulting / Management Consulting           7980\n",
      "Financial Services / Consulting                       5738\n",
      "Unknown/Not Specified                                 4597\n",
      "Healthcare Services                                   4146\n",
      "Digital Media / Creative Tech                         4010\n",
      "Retail Sector (B2B)                                   2943\n",
      "Retail Technology / Hospitality Technology            2837\n",
      "Digital Marketing Agency / Web Development            2667\n",
      "Non-Profits / Associations (B2B)                      2661\n",
      "EdTech / E-Learning                                   2462\n",
      "Public Sector / Government (B2B)                      2385\n",
      "Green Technology / Sustainability Services            2268\n",
      "Wholesale / Distribution                              2259\n",
      "Food & Beverage                                       1961\n",
      "Logistics Technology / Supply Chain Tech              1678\n",
      "HR / Recruitment                                      1375\n",
      "Service-Oriented (General B2B)                        1311\n",
      "Healthcare Technology / HealthTech                    1121\n",
      "Legal Technology                                      1091\n",
      "Telecommunications Services / Infrastructure           699\n",
      "Pharmaceutical Services / Consulting                   676\n",
      "Aerospace Technology / Space Tech                      603\n",
      "Cybersecurity Services                                 387\n",
      "Packaging Solutions                                    339\n",
      "Manufacturing Services / PaaS                          313\n",
      "Software Development / SaaS                            159\n",
      "Automation Technology                                  113\n",
      "Wellness Technology / Alternative Health Products       66\n",
      "Other                                                   64\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Finished Step 6 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 6d: Filling Remaining NaNs in Industry_Category_Standardized ---\")\n",
    "\n",
    "# Check if the 'Industry_Category_Standardized' column exists\n",
    "if 'Industry_Category_Standardized' not in df_processed_step3_categorization.columns:\n",
    "    print(\"'Industry_Category_Standardized' column not found. Ensure Step 6c (Apply Mapping) ran successfully.\")\n",
    "else:\n",
    "    # We are skipping the detailed SIC code mapping effort due to low potential impact (only 148 records).\n",
    "    # Directly proceed to fill any remaining NaNs from the primary industry mapping.\n",
    "    \n",
    "    current_nans = df_processed_step3_categorization['Industry_Category_Standardized'].isnull().sum()\n",
    "    if current_nans > 0:\n",
    "        print(f\"Found {current_nans} NaN values in 'Industry_Category_Standardized' before final fill (these include the 148 that had SICs).\")\n",
    "        df_processed_step3_categorization['Industry_Category_Standardized'] = (df_processed_step3_categorization['Industry_Category_Standardized'].fillna('Unknown/Not Specified')\n",
    ")\n",
    "        print(\"Filled remaining NaNs in 'Industry_Category_Standardized' with 'Unknown/Not Specified'.\")\n",
    "    else:\n",
    "        print(\"No NaN values found in 'Industry_Category_Standardized' to fill.\")\n",
    "\n",
    "    print(\"\\nValue counts for 'Industry_Category_Standardized' (final):\")\n",
    "    print(df_processed_step3_categorization['Industry_Category_Standardized'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n--- Finished Step 6 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cfed245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = df_processed_step3_categorization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dda19c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 7: Process Company Country / City ---\n",
      "\n",
      "--- Step 7a: Inspecting 'Company Country' and 'Company City' ---\n",
      "Number of unique 'Company Country' values: 3\n",
      "Sample unique 'Company Country' values (first 50):\n",
      "['Switzerland' 'Germany' 'Austria']\n",
      "\n",
      "Value counts for 'Company Country' (Top 20):\n",
      "Company Country\n",
      "Germany        58938\n",
      "Switzerland    18184\n",
      "Austria         6762\n",
      "Name: count, dtype: int64\n",
      "Missing 'Company Country' values: 0\n",
      "\n",
      "Number of unique 'Company City' values: 6251\n",
      "Missing 'Company City' values: 879\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Step 7: Process Company Country / City ---\")\n",
    "print(\"\\n--- Step 7a: Inspecting 'Company Country' and 'Company City' ---\")\n",
    "\n",
    "if 'Company Country' in df_large.columns:\n",
    "    print(f\"Number of unique 'Company Country' values: {df_large['Company Country'].nunique()}\")\n",
    "    print(\"Sample unique 'Company Country' values (first 50):\")\n",
    "    # astype(str) to handle potential mixed types or NaNs gracefully for unique() display\n",
    "    print(df_large['Company Country'].astype(str).unique()[:50])\n",
    "    print(\"\\nValue counts for 'Company Country' (Top 20):\")\n",
    "    print(df_large['Company Country'].value_counts(dropna=False).nlargest(20))\n",
    "    print(f\"Missing 'Company Country' values: {df_large['Company Country'].isnull().sum()}\")\n",
    "else:\n",
    "    print(\"'Company Country' column not found.\")\n",
    "\n",
    "if 'Company City' in df_large.columns:\n",
    "    # City can have many unique values, so we usually don't print all uniques\n",
    "    print(f\"\\nNumber of unique 'Company City' values: {df_large['Company City'].nunique()}\")\n",
    "    print(f\"Missing 'Company City' values: {df_large['Company City'].isnull().sum()}\")\n",
    "    # print(\"Sample 'Company City' values:\", df_large['Company City'].dropna().unique()[:20]) # Optional\n",
    "else:\n",
    "    print(\"'Company City' column not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a6efe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 7b: Standardizing Country Names (Simplified) ---\n",
      "Unique 'Country_Standardized' values (should be Germany, Switzerland, Austria):\n",
      "['Switzerland' 'Germany' 'Austria']\n",
      "No complex normalization needed due to clean source country data.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 7b: Standardizing Country Names (Simplified) ---\")\n",
    "if 'Company Country' in df_large.columns:\n",
    "    df_large['Country_Standardized'] = df_large['Company Country'] # Direct assignment as data is clean\n",
    "    print(\"Unique 'Country_Standardized' values (should be Germany, Switzerland, Austria):\")\n",
    "    print(df_large['Country_Standardized'].unique())\n",
    "    print(\"No complex normalization needed due to clean source country data.\")\n",
    "else:\n",
    "    print(\"'Company Country' column not found.\")\n",
    "    df_large['Country_Standardized'] = np.nan # Create column if it was missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7426a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 7c: Creating 'Geographic_Reach_Category_Standardized' (Simplified) ---\n",
      "\n",
      "Value counts for 'Geographic_Reach_Category_Standardized':\n",
      "Geographic_Reach_Category_Standardized\n",
      "National (Germany)        58938\n",
      "National (Switzerland)    18184\n",
      "National (Austria)         6762\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Finished Step 7 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 7c: Creating 'Geographic_Reach_Category_Standardized' (Simplified) ---\")\n",
    "\n",
    "# Your standardized reach categories:\n",
    "# \"National (Germany)\", \"National (Switzerland)\", \"National (Austria)\",\n",
    "# \"DACH Region\", \"Unknown / Not Specified\"\n",
    "# \"European\" and \"International / Global\" are not applicable if ONLY these 3 countries exist.\n",
    "\n",
    "def determine_geographic_reach_simplified(country_std):\n",
    "    if pd.isna(country_std): # Should not happen given your data, but good practice\n",
    "        return 'Unknown / Not Specified'\n",
    "\n",
    "    country_lower = str(country_std).lower()\n",
    "\n",
    "    if country_lower == 'germany':\n",
    "        return 'National (Germany)'\n",
    "    elif country_lower == 'switzerland':\n",
    "        return 'National (Switzerland)'\n",
    "    elif country_lower == 'austria':\n",
    "        return 'National (Austria)'\n",
    "    else:\n",
    "        # This case should ideally not be reached if your input is only these three\n",
    "        return 'Unknown / Not Specified'\n",
    "\n",
    "if 'Country_Standardized' not in df_large.columns:\n",
    "    print(\"'Country_Standardized' column not found. Run Step 7b first.\")\n",
    "    df_large['Geographic_Reach_Category_Standardized'] = 'Unknown / Not Specified' # Default\n",
    "else:\n",
    "    df_large['Geographic_Reach_Category_Standardized'] = df_large['Country_Standardized'].apply(determine_geographic_reach_simplified)\n",
    "\n",
    "    # --- Optional: Assigning \"DACH Region\" ---\n",
    "    # At this point, all companies are assigned a \"National (Country X)\" category.\n",
    "    # If your project definition considers ANY company from Germany, Austria, or Switzerland\n",
    "    # as being part of the \"DACH Region\" for broader categorization, you could do this:\n",
    "    #\n",
    "    # apply_dach_override = True # Set to False if you want to keep them separate National categories\n",
    "    # if apply_dach_override:\n",
    "    #     print(\"\\nOverriding National categories to 'DACH Region' for all G/A/S companies...\")\n",
    "    #     df_large['Geographic_Reach_Category_Standardized'] = 'DACH Region'\n",
    "    #\n",
    "    # More nuanced DACH assignment would require other data (e.g., if company description says \"we serve DACH\").\n",
    "    # For now, the above function assigns specific national categories.\n",
    "    # If you want a \"DACH Region\" category, you need to decide when it applies.\n",
    "    # If a company is ONLY in Germany, is it \"National (Germany)\" or \"DACH Region\" for your purposes?\n",
    "    # Your current 70 partner data has \"DACH Region\" as a category - how was that determined?\n",
    "    # If it means \"the company primarily operates within one or more of these three countries\n",
    "    # and its main focus is this collective region\", then a simple override might be too broad.\n",
    "    #\n",
    "    # Let's assume for now you want to keep them as \"National (Germany)\", \"National (Austria)\", \"National (Switzerland)\"\n",
    "    # unless a company explicitly states its reach is the broader DACH region in its description.\n",
    "    # We can refine DACH assignment later if needed, after processing descriptions.\n",
    "\n",
    "print(\"\\nValue counts for 'Geographic_Reach_Category_Standardized':\")\n",
    "print(df_large['Geographic_Reach_Category_Standardized'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n--- Finished Step 7 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7becbe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 8: Evaluate Short Description and SEO Description ---\n",
      "\n",
      "--- Step 8a: Inspecting 'Short Description' and 'SEO Description' Columns ---\n",
      "\n",
      "--- Inspecting 'Short Description' ---\n",
      "Data type of 'Short Description': object\n",
      "Missing 'Short Description' values: 12812 (out of 83884 total rows, 15.27%)\n",
      "\n",
      "Sample 'Short Description' values (first 5 non-null):\n",
      "['Vorhang auf für Ihren Event. Laden Sie ein, schaffen Sie Welten, verblüffen Sie Besucher, bringen Sie Ihr Publikum zum Staunen. Schöpfen Sie aus dem Vollen, inszenieren Sie Ihre Shows, Konzerte, Messen und Ausstellungen in den grosszügigen Eventräumen des TRAFO.\\n\\nZählen Sie für Ihren Event auf die weitreichende Erfahrung und das vertiefte Fachwissen unserer Veranstaltungstechniker und Eventplaner. Bespielen Sie Eventräume unterschiedlicher Grösse wie die TRAFOHALLE oder den Glassaal. Verwöhnen Sie Ihr Publikum mit den Köstlichkeiten unserer Eventkulinarik.\\n\\nNennen Sie die Ziele Ihres Events und nutzen Sie die Beratung unserer Spezialisten aus unzähligen Veranstaltungen. TRAFO Baden - für jeden Event stimmen Raum, Technik, Mobiliar und Dekoration.', 'Architektur - Planung und Bauleitung, Quartierplanverfahren, Projektentwicklungen, Wettbewerbe, Innenarchitektur, Visualisierungen\\n\\nLandschaftsarchitektur - Planung und Bauleitung', 'Treuhand und Immobilien mit Herz und Verstand. Sowohl für Unternehmen wie auch für Privatpersonen. Wir beraten nicht nur. Wir hinterfragen, analysieren und legen die Fakten auf den Tisch. Dabei spielt es keine Rolle, ob Sie uns administrative Arbeiten dauerhaft übertragen oder gezielt Unterstützung bei einer Aufgabe wünschen. Bei allem was wir tun steht stets der Mensch im Mittelpunkt. Unser Ziel ist immer Ihr langfristiger Erfolg.', 'WIR SIND IHR REGIONALER JOBPROFI.  \\nDie Firma SEELAND JOB AG hat ihren Sitz in Biel, Solothurn und Bern. \\n\\nUnser Team besteht aus Menschen mit Empathie, sozialer Erfahrung und Verantwortung.', 'We focus on our area of expertise: software development. Whether you are trying to create a website to process millions of records or a software to ease or expedite the process, our experience and know-how will ensure that we come up with the perfect solution, together.']\n",
      "\n",
      "--- Inspecting 'SEO Description' ---\n",
      "Data type of 'SEO Description': object\n",
      "Missing 'SEO Description' values: 37703 (out of 83884 total rows, 44.95%)\n",
      "\n",
      "Sample 'SEO Description' values (first 5 non-null):\n",
      "['Herzlich willkommen bei Otto Partner Architekten AG. Wir sind ein modernes Architektur-Büro in Liestal, BL.', 'Durchforschen Sie unsere aktuellen Stellenangebote und finden Temporär- & Feststellenvermittlung für Bau, Handwerk, Industrie, Kaufmännische Bereiche, Kaderstellen', 'TEDxZurich is an independently organised event.', 'Sourcing and roasting exceptional coffees, shipping worldwide for wholesale & retail or carefully brewed in our Berlin-based cake shops.', \"Salles et halles d'événement à Fribourg. Grand bâtiment capable de contenir jusqu'à 10.000 personnes. Entièrement personnalisable pour répondre à vos besoins.\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Step 8: Evaluate Short Description and SEO Description ---\")\n",
    "print(\"\\n--- Step 8a: Inspecting 'Short Description' and 'SEO Description' Columns ---\")\n",
    "\n",
    "# --- Inspect 'Short Description' ---\n",
    "if 'Short Description' in df_large.columns:\n",
    "    print(\"\\n--- Inspecting 'Short Description' ---\")\n",
    "    # Get basic info, including data type and non-null count\n",
    "    # df_large['Short Description'].info() # This prints to console, a bit verbose for this summary\n",
    "    print(f\"Data type of 'Short Description': {df_large['Short Description'].dtype}\")\n",
    "    missing_short_desc_count = df_large['Short Description'].isnull().sum()\n",
    "    total_rows = len(df_large)\n",
    "    print(f\"Missing 'Short Description' values: {missing_short_desc_count} (out of {total_rows} total rows, {((missing_short_desc_count/total_rows)*100 if total_rows > 0 else 0):.2f}%)\")\n",
    "    \n",
    "    # Display a few non-null samples to check content and quality\n",
    "    if missing_short_desc_count < total_rows: # If there's at least one non-null value\n",
    "        print(\"\\nSample 'Short Description' values (first 5 non-null):\")\n",
    "        # .dropna() to remove NaNs, then .head() to get first few\n",
    "        # .tolist() to print them cleanly\n",
    "        print(df_large['Short Description'].dropna().head(5).tolist())\n",
    "    else:\n",
    "        print(\"All 'Short Description' values are missing.\")\n",
    "else:\n",
    "    print(\"'Short Description' column not found.\")\n",
    "    # If the column is missing, create an empty one so downstream code doesn't break\n",
    "    # This ensures 'Combined_Description' can still be created.\n",
    "    df_large['Short Description'] = np.nan \n",
    "\n",
    "\n",
    "# --- Inspect 'SEO Description' ---\n",
    "if 'SEO Description' in df_large.columns:\n",
    "    print(\"\\n--- Inspecting 'SEO Description' ---\")\n",
    "    # df_large['SEO Description'].info() # Verbose\n",
    "    print(f\"Data type of 'SEO Description': {df_large['SEO Description'].dtype}\")\n",
    "    missing_seo_desc_count = df_large['SEO Description'].isnull().sum()\n",
    "    print(f\"Missing 'SEO Description' values: {missing_seo_desc_count} (out of {total_rows} total rows, {((missing_seo_desc_count/total_rows)*100 if total_rows > 0 else 0):.2f}%)\")\n",
    "\n",
    "    if missing_seo_desc_count < total_rows:\n",
    "        print(\"\\nSample 'SEO Description' values (first 5 non-null):\")\n",
    "        print(df_large['SEO Description'].dropna().head(5).tolist())\n",
    "    else:\n",
    "        print(\"All 'SEO Description' values are missing.\")\n",
    "else:\n",
    "    print(\"'SEO Description' column not found.\")\n",
    "    df_large['SEO Description'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "943d1df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 8b: Combining into a Single 'Combined_Description' Column ---\n",
      "\n",
      "Number of missing 'Combined_Description' values: 9179 (10.94%)\n",
      "\n",
      "Sample 'Combined_Description' values (first 5 non-null):\n",
      "['Vorhang auf für Ihren Event. Laden Sie ein, schaffen Sie Welten, verblüffen Sie Besucher, bringen Sie Ihr Publikum zum Staunen. Schöpfen Sie aus dem Vollen, inszenieren Sie Ihre Shows, Konzerte, Messen und Ausstellungen in den grosszügigen Eventräumen des TRAFO.\\n\\nZählen Sie für Ihren Event auf die weitreichende Erfahrung und das vertiefte Fachwissen unserer Veranstaltungstechniker und Eventplaner. Bespielen Sie Eventräume unterschiedlicher Grösse wie die TRAFOHALLE oder den Glassaal. Verwöhnen Sie Ihr Publikum mit den Köstlichkeiten unserer Eventkulinarik.\\n\\nNennen Sie die Ziele Ihres Events und nutzen Sie die Beratung unserer Spezialisten aus unzähligen Veranstaltungen. TRAFO Baden - für jeden Event stimmen Raum, Technik, Mobiliar und Dekoration.', 'Architektur - Planung und Bauleitung, Quartierplanverfahren, Projektentwicklungen, Wettbewerbe, Innenarchitektur, Visualisierungen\\n\\nLandschaftsarchitektur - Planung und Bauleitung. Herzlich willkommen bei Otto Partner Architekten AG. Wir sind ein modernes Architektur-Büro in Liestal, BL.', 'Treuhand und Immobilien mit Herz und Verstand. Sowohl für Unternehmen wie auch für Privatpersonen. Wir beraten nicht nur. Wir hinterfragen, analysieren und legen die Fakten auf den Tisch. Dabei spielt es keine Rolle, ob Sie uns administrative Arbeiten dauerhaft übertragen oder gezielt Unterstützung bei einer Aufgabe wünschen. Bei allem was wir tun steht stets der Mensch im Mittelpunkt. Unser Ziel ist immer Ihr langfristiger Erfolg.', 'WIR SIND IHR REGIONALER JOBPROFI.  \\nDie Firma SEELAND JOB AG hat ihren Sitz in Biel, Solothurn und Bern. \\n\\nUnser Team besteht aus Menschen mit Empathie, sozialer Erfahrung und Verantwortung.. Durchforschen Sie unsere aktuellen Stellenangebote und finden Temporär- & Feststellenvermittlung für Bau, Handwerk, Industrie, Kaufmännische Bereiche, Kaderstellen', 'We focus on our area of expertise: software development. Whether you are trying to create a website to process millions of records or a software to ease or expedite the process, our experience and know-how will ensure that we come up with the perfect solution, together.']\n",
      "\n",
      "Sample 'Combined_Description' values (longer examples, if any):\n",
      "['Vorhang auf für Ihren Event. Laden Sie ein, schaffen Sie Welten, verblüffen Sie Besucher, bringen Sie Ihr Publikum zum Staunen. Schöpfen Sie aus dem Vollen, inszenieren Sie Ihre Shows, Konzerte, Messen und Ausstellungen in den grosszügigen Eventräumen des TRAFO.\\n\\nZählen Sie für Ihren Event auf die weitreichende Erfahrung und das vertiefte Fachwissen unserer Veranstaltungstechniker und Eventplaner. Bespielen Sie Eventräume unterschiedlicher Grösse wie die TRAFOHALLE oder den Glassaal. Verwöhnen Sie Ihr Publikum mit den Köstlichkeiten unserer Eventkulinarik.\\n\\nNennen Sie die Ziele Ihres Events und nutzen Sie die Beratung unserer Spezialisten aus unzähligen Veranstaltungen. TRAFO Baden - für jeden Event stimmen Raum, Technik, Mobiliar und Dekoration.', 'Architektur - Planung und Bauleitung, Quartierplanverfahren, Projektentwicklungen, Wettbewerbe, Innenarchitektur, Visualisierungen\\n\\nLandschaftsarchitektur - Planung und Bauleitung. Herzlich willkommen bei Otto Partner Architekten AG. Wir sind ein modernes Architektur-Büro in Liestal, BL.', 'Treuhand und Immobilien mit Herz und Verstand. Sowohl für Unternehmen wie auch für Privatpersonen. Wir beraten nicht nur. Wir hinterfragen, analysieren und legen die Fakten auf den Tisch. Dabei spielt es keine Rolle, ob Sie uns administrative Arbeiten dauerhaft übertragen oder gezielt Unterstützung bei einer Aufgabe wünschen. Bei allem was wir tun steht stets der Mensch im Mittelpunkt. Unser Ziel ist immer Ihr langfristiger Erfolg.']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 8b: Combining into a Single 'Combined_Description' Column ---\")\n",
    "\n",
    "# Ensure the columns exist (they would have been created as NaN columns in 8a if missing)\n",
    "# Fill NaNs with empty strings for safe concatenation\n",
    "s_desc_clean = df_large['Short Description'].fillna('').astype(str)\n",
    "seo_desc_clean = df_large['SEO Description'].fillna('').astype(str)\n",
    "\n",
    "# Create the 'Combined_Description'\n",
    "# Strategy:\n",
    "# 1. If Short Description is substantial, use it.\n",
    "# 2. If Short Description is short or missing, and SEO Description is substantial, use SEO Description.\n",
    "# 3. If both are substantial, concatenate them with a separator.\n",
    "# 4. If one is substantial and the other is short but different, concatenate.\n",
    "\n",
    "df_large['Combined_Description'] = '' # Initialize\n",
    "\n",
    "for index, row in df_large.iterrows():\n",
    "    s_desc = str(row['Short Description']).strip() if pd.notna(row['Short Description']) else ''\n",
    "    seo_desc = str(row['SEO Description']).strip() if pd.notna(row['SEO Description']) else ''\n",
    "    \n",
    "    combined = ''\n",
    "    # Check if descriptions are substantially different to avoid near-duplicate concatenation\n",
    "    # This is a simple check; more sophisticated similarity could be used but adds complexity.\n",
    "    are_different_enough = True\n",
    "    if s_desc and seo_desc:\n",
    "        # A simple heuristic: if one is contained in the other (ignoring case and spaces),\n",
    "        # consider them not different enough to just append.\n",
    "        s_desc_norm = ''.join(s_desc.lower().split())\n",
    "        seo_desc_norm = ''.join(seo_desc.lower().split())\n",
    "        if s_desc_norm in seo_desc_norm or seo_desc_norm in s_desc_norm:\n",
    "            are_different_enough = False\n",
    "\n",
    "    if s_desc and seo_desc:\n",
    "        if are_different_enough or len(s_desc) < 50 or len(seo_desc) < 50: # Concatenate if different or if one is short\n",
    "            combined = s_desc + \". \" + seo_desc # Using \". \" as a separator\n",
    "        else: # Both are long and similar, prioritize the longer one or Short Description\n",
    "            combined = s_desc if len(s_desc) >= len(seo_desc) else seo_desc\n",
    "    elif s_desc:\n",
    "        combined = s_desc\n",
    "    elif seo_desc:\n",
    "        combined = seo_desc\n",
    "    \n",
    "    df_large.loc[index, 'Combined_Description'] = combined.strip()\n",
    "\n",
    "# Replace fully empty strings in 'Combined_Description' with np.nan\n",
    "df_large['Combined_Description'] = df_large['Combined_Description'].replace('', np.nan)\n",
    "\n",
    "missing_combined_desc = df_large['Combined_Description'].isnull().sum()\n",
    "print(f\"\\nNumber of missing 'Combined_Description' values: {missing_combined_desc} ({((missing_combined_desc/total_rows)*100 if total_rows > 0 else 0):.2f}%)\")\n",
    "\n",
    "if missing_combined_desc < total_rows:\n",
    "    print(\"\\nSample 'Combined_Description' values (first 5 non-null):\")\n",
    "    print(df_large['Combined_Description'].dropna().head(5).tolist())\n",
    "    # Also print a few longer ones to see the concatenation effect\n",
    "    print(\"\\nSample 'Combined_Description' values (longer examples, if any):\")\n",
    "    long_descriptions = df_large[df_large['Combined_Description'].str.len() > 150]['Combined_Description'].dropna()\n",
    "    if not long_descriptions.empty:\n",
    "        print(long_descriptions.head(3).tolist())\n",
    "    else:\n",
    "        print(\"No long combined descriptions found (length > 150).\")\n",
    "else:\n",
    "    print(\"All 'Combined_Description' values are missing (or columns were not found).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec4f9266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 8c: Assessing Language of 'Combined_Description' ---\n",
      "Assessing language for a sample of 'Combined_Description' (up to 1000 rows)...\n",
      "\n",
      "Language distribution in the sampled 'Combined_Description':\n",
      "description_language_sample\n",
      "NaN                  82884\n",
      "de                     505\n",
      "en                     431\n",
      "fr                      47\n",
      "it                       7\n",
      "unknown_too_short        3\n",
      "es                       3\n",
      "lt                       1\n",
      "ja                       1\n",
      "nl                       1\n",
      "ar                       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Finished Step 8 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 8c: Assessing Language of 'Combined_Description' ---\")\n",
    "\n",
    "if 'Combined_Description' not in df_large.columns or df_large['Combined_Description'].isnull().all():\n",
    "    print(\"'Combined_Description' column is missing or all values are NaN. Cannot assess language.\")\n",
    "else:\n",
    "    try:\n",
    "        # You might need to install langdetect: pip install langdetect\n",
    "        from langdetect import detect, LangDetectException, DetectorFactory\n",
    "        DetectorFactory.seed = 0 # For reproducible results if langdetect uses randomness\n",
    "\n",
    "        def detect_language_robust(text):\n",
    "            if pd.isna(text) or not str(text).strip(): # Ensure text is string and not just whitespace\n",
    "                return 'unknown_empty_or_nan'\n",
    "            try:\n",
    "                # langdetect works best with longer texts.\n",
    "                # If text is very short, it might be unreliable.\n",
    "                if len(str(text).strip()) < 15: # Arbitrary threshold for very short text\n",
    "                    return 'unknown_too_short'\n",
    "                return detect(str(text))\n",
    "            except LangDetectException:\n",
    "                # This can happen if the text is ambiguous or contains no detectable language features\n",
    "                return 'unknown_error_detecting'\n",
    "\n",
    "        print(\"Assessing language for a sample of 'Combined_Description' (up to 1000 rows)...\")\n",
    "        # Take a sample of non-NaN descriptions\n",
    "        # Ensure we only sample from rows where Combined_Description is not NaN\n",
    "        descriptions_for_lang_detect = df_large['Combined_Description'].dropna()\n",
    "        \n",
    "        if not descriptions_for_lang_detect.empty:\n",
    "            sample_size = min(1000, len(descriptions_for_lang_detect))\n",
    "            sampled_descriptions = descriptions_for_lang_detect.sample(n=sample_size, random_state=42) # random_state for reproducibility\n",
    "            \n",
    "            # Apply language detection to the sample\n",
    "            # It's good practice to convert to string explicitly if there's any doubt about dtype\n",
    "            df_large.loc[sampled_descriptions.index, 'description_language_sample'] = sampled_descriptions.astype(str).apply(detect_language_robust)\n",
    "            \n",
    "            print(\"\\nLanguage distribution in the sampled 'Combined_Description':\")\n",
    "            print(df_large['description_language_sample'].value_counts(dropna=False))\n",
    "\n",
    "            # Optional: If you want to apply to the whole dataset (can be slow for ~80k rows)\n",
    "            # print(\"\\nApplying language detection to the entire dataset (this may take time)...\")\n",
    "            # df_large['description_language_full'] = df_large['Combined_Description'].astype(str).apply(detect_language_robust)\n",
    "            # print(\"\\nLanguage distribution in the full 'Combined_Description' dataset:\")\n",
    "            # print(df_large['description_language_full'].value_counts(dropna=False))\n",
    "        else:\n",
    "            print(\"No non-NaN descriptions available for language detection.\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"The 'langdetect' library is not installed. Please install it: pip install langdetect\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during language detection: {e}\")\n",
    "\n",
    "print(\"\\n--- Finished Step 8 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2720ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed DataFrame to a new CSV file\n",
    "# output_file_path = 'final_80k.csv'\n",
    "# df_large.to_csv(output_file_path, index=False)\n",
    "# print(f\"\\nProcessed DataFrame saved to '{output_file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9e8f7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 9: Process 'Founded Year' ---\n",
      "\n",
      "--- Step 9a: Inspecting and Cleaning 'Founded Year' ---\n",
      "Original 'Founded Year' data type: float64\n",
      "Sample unique 'Founded Year' values (first 30): ['2003.0' '1907.0' 'nan' '2008.0' '1996.0' '2010.0' '1999.0' '2001.0'\n",
      " '2002.0' '2012.0' '1991.0' '1882.0' '1979.0' '2020.0' '1941.0' '1953.0'\n",
      " '1962.0' '2005.0' '1892.0' '2017.0' '2014.0' '2019.0' '1987.0' '1972.0'\n",
      " '1981.0' '1990.0' '1952.0' '1967.0' '2006.0' '1902.0']\n",
      "Missing 'Founded Year' values: 24145 (out of 83884 total rows, 28.78%)\n",
      "\n",
      "Missing values in 'founded_year_numeric' after cleaning: 24145\n",
      "Descriptive statistics for 'founded_year_numeric':\n",
      "count        59739.0\n",
      "mean     1982.300875\n",
      "std        43.787271\n",
      "min           1502.0\n",
      "25%           1970.0\n",
      "50%           1997.0\n",
      "75%           2011.0\n",
      "max           2024.0\n",
      "Name: founded_year_numeric, dtype: Float64\n",
      "Min founded year: 1502, Max founded year: 2024\n",
      "WARNING: Found 269 companies with founding year before 1800.\n",
      "Setting 269 founding years before 1800 to NA.\n",
      "\n",
      "Missing values in 'founded_year_numeric' after handling unrealistic years: 24414\n",
      "Descriptive statistics for 'founded_year_numeric' after handling unrealistic years:\n",
      "count    59470.000000\n",
      "mean      1983.504439\n",
      "std         39.718247\n",
      "min       1800.000000\n",
      "25%       1971.000000\n",
      "50%       1997.000000\n",
      "75%       2011.000000\n",
      "max       2024.000000\n",
      "Name: founded_year_numeric, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # For np.nan if 'Founded Year' is missing for some\n",
    "from datetime import datetime  # To get the current year\n",
    "\n",
    "print(\"\\n--- Step 9: Process 'Founded Year' ---\")\n",
    "print(\"\\n--- Step 9a: Inspecting and Cleaning 'Founded Year' ---\")\n",
    "\n",
    "if 'Founded Year' not in df_large.columns:\n",
    "    print(\"'Founded Year' column not found. Skipping this step.'\")\n",
    "    # If you intend to use logic based on founded_year_numeric later,\n",
    "    # you might want to create it as a column of NaNs here to prevent errors.\n",
    "    # df_large['founded_year_numeric'] = np.nan\n",
    "else:\n",
    "    print(f\"Original 'Founded Year' data type: {df_large['Founded Year'].dtype}\")\n",
    "    \n",
    "    # Display a sample of unique values to understand its current format\n",
    "    # astype(str) to handle potential mixed types or NaNs gracefully for unique() display\n",
    "    unique_founded_years = df_large['Founded Year'].astype(str).unique()\n",
    "    print(f\"Sample unique 'Founded Year' values (first 30): {unique_founded_years[:30]}\")\n",
    "    \n",
    "    missing_founded_year_count = df_large['Founded Year'].isnull().sum()\n",
    "    total_rows = len(df_large)\n",
    "    print(f\"Missing 'Founded Year' values: {missing_founded_year_count} (out of {total_rows} total rows, {((missing_founded_year_count/total_rows)*100 if total_rows > 0 else 0):.2f}%)\")\n",
    "\n",
    "    # Convert to numeric. Years should ideally be integers.\n",
    "    # pd.to_numeric will turn non-convertible values into NaT/NaN if errors='coerce'\n",
    "    # We'll then try to extract the year if it's a datetime-like object or just keep numeric.\n",
    "    \n",
    "    # Attempt 1: Direct to_numeric (if years are already numbers or simple strings like \\\"2010\\\")\n",
    "    df_large['founded_year_numeric'] = pd.to_numeric(df_large['Founded Year'], errors='coerce')\n",
    "\n",
    "    # Attempt 2: For cases where 'Founded Year' might be full dates (e.g., \\\"2010-01-15\\\")\n",
    "    # This will extract the year part. Only apply where direct to_numeric failed.\n",
    "    # This also helps if some are datetimes and some are just years as floats/ints from Excel.\n",
    "    if df_large['founded_year_numeric'].isnull().any():  # If some failed direct numeric conversion\n",
    "        # Try converting to datetime and extracting the year\n",
    "        # This is robust to various date formats Pandas can infer\n",
    "        potential_datetimes = pd.to_datetime(df_large['Founded Year'], errors='coerce')\n",
    "        # Fill NaNs in 'founded_year_numeric' with years extracted from these datetimes\n",
    "        df_large['founded_year_numeric'] = df_large['founded_year_numeric'].fillna(potential_datetimes.dt.year)\n",
    "\n",
    "    # Final conversion to integer type that supports NaNs (Pandas nullable integer)\n",
    "    # Before converting to Int64, ensure all non-NaNs are whole numbers (no decimals from .dt.year if it was a float year)\n",
    "    # This also handles cases where 'Founded Year' might have been like 2010.0 from Excel\n",
    "    df_large['founded_year_numeric'] = df_large['founded_year_numeric'].round(0).astype('Int64')\n",
    "\n",
    "    print(f\"\\nMissing values in 'founded_year_numeric' after cleaning: {df_large['founded_year_numeric'].isnull().sum()}\")\n",
    "    print(\"Descriptive statistics for 'founded_year_numeric':\")\n",
    "    print(df_large['founded_year_numeric'].describe())\n",
    "\n",
    "    # Check for unrealistic years (e.g., future years, very old years)\n",
    "    if df_large['founded_year_numeric'].notna().any():\n",
    "        current_year_val = datetime.now().year\n",
    "        print(f\"Min founded year: {df_large['founded_year_numeric'].min()}, Max founded year: {df_large['founded_year_numeric'].max()}\")\n",
    "        unrealistic_future_years = df_large[df_large['founded_year_numeric'] > current_year_val]['founded_year_numeric'].count()\n",
    "        unrealistic_old_years = df_large[df_large['founded_year_numeric'] < 1800]['founded_year_numeric'].count()  # Arbitrary old threshold\n",
    "        if unrealistic_future_years > 0:\n",
    "            print(f\"WARNING: Found {unrealistic_future_years} companies with founding year in the future.\")\n",
    "        if unrealistic_old_years > 0:\n",
    "            print(f\"WARNING: Found {unrealistic_old_years} companies with founding year before 1800.\")\n",
    "            # You might want to set these unrealistic years to NaN:\n",
    "            # df_large.loc[df_large['founded_year_numeric'] > current_year_val, 'founded_year_numeric'] = pd.NA\n",
    "            # df_large.loc[df_large['founded_year_numeric'] < 1800, 'founded_year_numeric'] = pd.NA\n",
    "\n",
    "# In your Step 9a, after printing the warnings:\n",
    "current_year_val = datetime.now().year # Ensure this is defined in scope\n",
    "\n",
    "# Define your threshold for \"too old\"\n",
    "too_old_threshold = 1800 # Or 1700, or 1850, depending on your judgment for this dataset\n",
    "\n",
    "condition_too_old = df_large['founded_year_numeric'] < too_old_threshold\n",
    "num_set_to_na_old = condition_too_old.sum()\n",
    "if num_set_to_na_old > 0:\n",
    "    print(f\"Setting {num_set_to_na_old} founding years before {too_old_threshold} to NA.\")\n",
    "    df_large.loc[condition_too_old, 'founded_year_numeric'] = pd.NA\n",
    "\n",
    "# Also check for future years again, just in case (though max is 2024 here)\n",
    "condition_future = df_large['founded_year_numeric'] > current_year_val\n",
    "num_set_to_na_future = condition_future.sum()\n",
    "if num_set_to_na_future > 0:\n",
    "    print(f\"Setting {num_set_to_na_future} founding years after {current_year_val} to NA.\")\n",
    "    df_large.loc[condition_future, 'founded_year_numeric'] = pd.NA\n",
    "    \n",
    "# Re-calculate missing count and describe after cleaning unrealistic years\n",
    "print(f\"\\nMissing values in 'founded_year_numeric' after handling unrealistic years: {df_large['founded_year_numeric'].isnull().sum()}\")\n",
    "print(\"Descriptive statistics for 'founded_year_numeric' after handling unrealistic years:\")\n",
    "# Describe might behave better if we ensure it's float for describe if Int64 causes issues with pd.NA\n",
    "if df_large['founded_year_numeric'].notna().any():\n",
    "    print(df_large['founded_year_numeric'].astype(float).describe())\n",
    "else:\n",
    "    print(\"founded_year_numeric is all NaN after handling unrealistic years.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ad0ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 9b: Using 'Founded Year' for Further Categorization/Features ---\n",
      "Identified 0 potential startups to update in 'Company_Size_Category'.\n",
      "\n",
      "Value counts for 'Company_Size_Category' after 'Startup' refinement:\n",
      "Company_Size_Category\n",
      "11-50                    64657\n",
      "51-250                   18748\n",
      "Startup                    443\n",
      "1-10                        27\n",
      "Unknown/Not Specified        6\n",
      "251-1000                     3\n",
      "1001+                        0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Descriptive statistics for 'Company_Age':\n",
      "count    59470.000000\n",
      "mean        41.495561\n",
      "std         39.718247\n",
      "min          1.000000\n",
      "25%         14.000000\n",
      "50%         28.000000\n",
      "75%         54.000000\n",
      "max        225.000000\n",
      "Name: Company_Age, dtype: float64\n",
      "\n",
      "--- Finished Step 9 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 9b: Using 'Founded Year' for Further Categorization/Features ---\")\n",
    "\n",
    "if 'founded_year_numeric' not in df_large.columns or df_large['founded_year_numeric'].isnull().all():\n",
    "    print(\"'founded_year_numeric' column not available or all NaN. Skipping refinement based on founding year.\")\n",
    "else:\n",
    "    current_year = datetime.now().year\n",
    "\n",
    "    # --- Option 1: Refine 'Company_Size_Category' to identify 'Startup' ---\n",
    "    # This overwrites the existing 'Company_Size_Category' for certain conditions.\n",
    "    # Ensure 'Company_Size_Category' column exists from previous steps.\n",
    "    if 'Company_Size_Category' in df_large.columns:\n",
    "        # Define what constitutes a startup based on age and current size category\n",
    "        # For example, founded in the last 3 years (current_year, year-1, year-2)\n",
    "        # AND currently categorized as 'Micro', 'Small', or 'Unknown/Not Specified'\n",
    "        \n",
    "        # Condition for being a young company\n",
    "        is_young_company_mask = (df_large['founded_year_numeric'] >= (current_year - 2)) & \\\n",
    "                                (df_large['founded_year_numeric'] <= current_year) # Founded in current year, year-1, or year-2\n",
    "        \n",
    "        # Condition for current size being small or unknown\n",
    "        is_small_or_unknown_size_mask = df_large['Company_Size_Category'].isin(['Micro', 'Small', '1-10', '11-50', 'Unknown/Not Specified'])\n",
    "        # Adjust '1-10', '11-50' if your labels from pd.cut for employees are different\n",
    "        \n",
    "        # Combine conditions\n",
    "        startup_condition_mask = is_young_company_mask & is_small_or_unknown_size_mask\n",
    "        \n",
    "        num_potential_startups = startup_condition_mask.sum()\n",
    "        print(f\"Identified {num_potential_startups} potential startups to update in 'Company_Size_Category'.\")\n",
    "\n",
    "        # Before overwriting, ensure 'Startup' is a valid category in 'Company_Size_Category'\n",
    "        if isinstance(df_large['Company_Size_Category'].dtype, pd.CategoricalDtype):\n",
    "            if 'Startup' not in df_large['Company_Size_Category'].cat.categories:\n",
    "                print(\"Adding 'Startup' to Company_Size_Category categories.\")\n",
    "                df_large['Company_Size_Category'] = df_large['Company_Size_Category'].cat.add_categories(['Startup'])\n",
    "        \n",
    "        df_large.loc[startup_condition_mask, 'Company_Size_Category'] = 'Startup'\n",
    "        \n",
    "        print(\"\\nValue counts for 'Company_Size_Category' after 'Startup' refinement:\")\n",
    "        print(df_large['Company_Size_Category'].value_counts(dropna=False))\n",
    "    else:\n",
    "        print(\"'Company_Size_Category' column not found. Cannot refine for startups.\")\n",
    "\n",
    "    # --- Option 2: Create a 'Company_Age' column ---\n",
    "    df_large['Company_Age'] = current_year - df_large['founded_year_numeric']\n",
    "    print(\"\\nDescriptive statistics for 'Company_Age':\")\n",
    "    # .describe() on a nullable integer series can sometimes be problematic if it contains pd.NA\n",
    "    # So, let's describe the non-NA values if that happens.\n",
    "    if df_large['Company_Age'].notna().any() and pd.api.types.is_integer_dtype(df_large['Company_Age']):\n",
    "         print(df_large['Company_Age'].astype(float).describe()) # Convert to float for describe to avoid potential IntNA issues\n",
    "    elif df_large['Company_Age'].notna().any():\n",
    "         print(df_large['Company_Age'].describe())\n",
    "    else:\n",
    "        print(\"Company_Age column is all NaN.\")\n",
    "\n",
    "\n",
    "    # --- Option 3: Create an 'Is_Startup' boolean column (alternative to Option 1) ---\n",
    "    # This keeps 'Company_Size_Category' based purely on employee/revenue,\n",
    "    # and 'Is_Startup' is a separate flag.\n",
    "    # Define startup_condition_mask as above if not already defined\n",
    "    # df_large['Is_Startup'] = False # Initialize\n",
    "    # df_large.loc[startup_condition_mask, 'Is_Startup'] = True\n",
    "    # print(\"\\nValue counts for 'Is_Startup':\")\n",
    "    # print(df_large['Is_Startup'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n--- Finished Step 9 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f861c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final DataFrame Structure ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 83884 entries, 0 to 84895\n",
      "Data columns (total 35 columns):\n",
      " #   Column                                  Non-Null Count  Dtype   \n",
      "---  ------                                  --------------  -----   \n",
      " 0   Company                                 83884 non-null  object  \n",
      " 1   # Employees                             83878 non-null  float64 \n",
      " 2   Industry                                79287 non-null  object  \n",
      " 3   Website                                 72080 non-null  object  \n",
      " 4   Company Linkedin Url                    83701 non-null  object  \n",
      " 5   Facebook Url                            31006 non-null  object  \n",
      " 6   Twitter Url                             13962 non-null  object  \n",
      " 7   Company Street                          63033 non-null  object  \n",
      " 8   Company City                            83005 non-null  object  \n",
      " 9   Company State                           83344 non-null  object  \n",
      " 10  Company Country                         83884 non-null  object  \n",
      " 11  Company Postal Code                     67849 non-null  float64 \n",
      " 12  Company Address                         83884 non-null  object  \n",
      " 13  Keywords                                46561 non-null  object  \n",
      " 14  Company Phone                           56628 non-null  object  \n",
      " 15  SEO Description                         46181 non-null  object  \n",
      " 16  Technologies                            73725 non-null  object  \n",
      " 17  Total Funding                           1918 non-null   float64 \n",
      " 18  Latest Funding                          2353 non-null   object  \n",
      " 19  Latest Funding Amount                   1796 non-null   float64 \n",
      " 20  Last Raised At                          2353 non-null   object  \n",
      " 21  Annual Revenue                          11164 non-null  float64 \n",
      " 22  SIC Codes                               5956 non-null   object  \n",
      " 23  Short Description                       71072 non-null  object  \n",
      " 24  Founded Year                            59739 non-null  float64 \n",
      " 25  employees_numeric                       83878 non-null  float64 \n",
      " 26  Company_Size_Category                   83884 non-null  category\n",
      " 27  revenue_numeric                         11164 non-null  float64 \n",
      " 28  Industry_Category_Standardized          83884 non-null  object  \n",
      " 29  Country_Standardized                    83884 non-null  object  \n",
      " 30  Geographic_Reach_Category_Standardized  83884 non-null  object  \n",
      " 31  Combined_Description                    74705 non-null  object  \n",
      " 32  description_language_sample             1000 non-null   object  \n",
      " 33  founded_year_numeric                    59470 non-null  Int64   \n",
      " 34  Company_Age                             59470 non-null  Int64   \n",
      "dtypes: Int64(2), category(1), float64(8), object(24)\n",
      "memory usage: 24.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display final DataFrame structure and columns\n",
    "print(\"\\n--- Final DataFrame Structure ---\")    \n",
    "print(df_large.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
